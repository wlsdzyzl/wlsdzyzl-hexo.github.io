---
title: 信息论——连续随机变量的熵和互信息
date: 2018-11-01 15:43:56
tags: [information theory]
categories: 信息论
mathjax: true
---
如何将离散随机变量的这些概念推广到连续随机变量？
<!--more-->

使用黎曼积分，我们可以得到：

$$
\begin{align}
H(X) &= -\sum_{x} p(x)\Delta x \log p(x) \Delta x\\
&= -\sum_{x}p(x)\log p(x)\Delta x - -\sum_{x}p(x)\log \Delta x\Delta x 
\end{align}
$$

上式中，最后一项是趋于负无穷的。

这意味着连续随机变量包含的信息是无穷的。但是无穷的是无法研究的，因此香农重新给了一个微分熵的定义，它在数学上不够严格，但是在实际上却非常有用。

$$
h(X) = \int _{-\infty }^{+\infty} p(x)\log p(x) dx
$$

可以看到它在形式上与离散形式的熵是非常相似的。

同时也有联合熵：
$$
h(X,Y) = -\int p(x,y)\log p(x,y) dxdy
$$

条件熵：
$$
h(X|Y) = -\iint p(x,y) \log p(x|y) dxdy = -\int p(y) \int p(x|y) \log p(x|y) dx dy
$$

不等式关系：
$
h(X,Y) = h(X) + h(Y|X) = h(Y) + h(X|Y)
$

$
h(X|Y) \leq h(X), h(Y|X) \leq h(Y)
$

$
h(X,Y)\leq h(X) + h(Y)
$

这些不等式都是存在的，与离散形式一致，但是要注意的是h(X)不一定是非负的了。

例如：
$$
X:p(x) = \left \{ \begin{array}{c} 
    \frac 1 {b-a} , a \leq x \leq b;\\
    0, otherwise;
    \end{array}
    \right.
$$

那么它的微分熵实际上等于$\log(b-a)$.当$b-a<1$的时候，这个熵是小于0的。

### 高斯分布的微分熵 ###

高斯分布概率密度如下：

$X:p(x) = \frac{1}{\sqrt {2 \pi \sigma} } exp [-\frac{(x-m)^2}{2\sigma ^2}]$

而它的微分熵为$h(x) = \frac 1 2 \log 2 \pi e \sigma^2$.

这个需要记住。当然只要带进定义就可以推算出来的。值得注意的是，它的微分熵和m（期望）是无关的

给定m和$\sigma$的情况下，当连续变量服从高斯分布的时候，微分熵最大。

### 互信息 ###

$I(X;Y) = \iint p(x,y) \log \frac{p(x,y)}{p(x)p(y)}dxdy$

可以直接使用黎曼积分得到，与离散的情况也非常一致。