---
title: 数学——Lagrange Multiplier
date: 2018-10-09 13:09:09
tags: [mathematics]
categories: 数学
mathjax: true
---

拉格朗日乘数法，是我们大学或者考研过来的耳熟能详的名词了。我们接触他的时候，应该是在求条件极值的时候。
<!--more-->
求$f(x,y)$在$g(x,y)=0$的条件下的极值。需要利用拉格朗日乘数构造新的式子：

$w(x,y,\lambda ) = f(x,y)+\lambda g(x,y)$

让$w(x,y,\lambda)$分别对$x,y,\lambda$求偏导，并令其为0：
$$
\left \{
\begin{array}{}
    w'_x(x,y,\lambda) = f'_x(x,y)+\lambda g'_x(x,y) = 0\\
    w'_y(x,y,\lambda) = f'_y(x,y)+ \lambda g'_y(x,y) = 0\\
    w'_{\lambda}(x,y,\lambda) = g(x,y) = 0
\end{array}
\right .
$$

不过大学的时候，我虽然会这么计算，但是却不知道原理。现在希望从原理解释一下，为什么要这么算。

暂时我想到了两种解释办法：

1.我们首先要知道的一个前提是，$g(x,y) = 0$在任意一点$(x_0,y_0)$切线的法向量为$(g'_x(x_0,y_0),g'_y(x_0,y_0))$.想象现在有一个点在约束的这条线上移动，为了找到让$f(x,y)$值最小的点，首先我们求出$f(x,y)$在当前点的梯度，梯度也就是朝着这个方向（或者反方向）前进，$f(x,y)$的值会变大（或者变小）,因此只要梯度与这条线上该点的法向量不平行，我们总是可以将梯度投影到该点的切线上，也就是依然能抄着切线的某个方向运动，让$f(x,y)$的值变小（如果要找到最大值，就是变大）。当梯度与法向量平行的时候，不论朝着哪个方向，都无法让$f(x,y)$的值变小，因此这个点就是一个极值小点。

而我们知道的，上面的法向量，实际上是$g(x,y)$在该点的梯度，因此我们有在极值点的时候：
 $$
 \nabla f(x,y) = \lambda \nabla g(x,y)
 $$
上式中，$\lambda$的正负取决于要的是极大值点还是极小值点。而上面的式子，实际上就与方程组的前两个方程一致。

2.另一个办法，画出等高线图。

![](https://evolution-video.oss-cn-beijing.aliyuncs.com/images/2012101621500549.png)
 
可以比较清晰地看出来，当$g(x,y)=0$与等高线相切的点是极小值。因为你只能在$g(x,y)=0$的这条线上移动，那么别的地方总会比它大（或者大）。那怎么求相切的部分的点呢？首先，等高线的方程，实际上是曲线到(x,y)平面的投影，方程为$f(x,y)=C$,同样的，既然相切，那么他们在该点的法向量一定是平行的。计算法向量，又回到了前面的内容：
$$
  \nabla f(x,y) = \lambda \nabla g(x,y)
$$
联立$g(x,y)=0$即可得到原来的方程组。

现在我们知道了条件极值如何解出来，但是这只是拉格朗日乘数法的一部分。拉格朗日乘数法是一种寻找变量受一个或多个条件所限制的多元函数的极值的方法。首先上面的条件限制只有一个，另外上面的条件也很简单，是在$g(x,y)=0$，如果我们要的是$g(x,y) \leq 0$的呢？

如果$f(x,y)$极值点本身就在上面的约束范围内，那么相当于没有约束，也就是$\lambda = 0$，否则在边界上，又回到了上面的问题：$g(x,y)=0$.总之，$\lambda g(x,y) = 0$.因此我们需要改变联立条件即可。

上面的式子，都是$\lambda$也会变化的情况。但是，在机器学习的正则化中，我们往往给出的式子是形如这样$E_{in} + \frac \lambda N \VertW\Vert^2$.而且这个$\lambda$可能是个定值。如果$\lambda$固定，得到的又是什么值？

我们可以推断出来的是如果$E_{in}$找到了最小的地方，那么$W$也就为0，否则$\nabla E_{in}$与$W$的比值是一个定值。从这些里得不到很有用的信息。但是从另一个角度来说，它确实限制了$W$的大小，虽然目前不知道具体限定到哪个范围。$\VertW_{reg}\Vert \leq \VertW_{lin}\Vert$是一定的，可以从反证法证明：如果$\VertW_{lin}\Vert<\VertW_{reg}\Vert$,那么$E_{in}$也会小，正则项也更小，所以这时候的$E_{reg}$比使用$W_{reg}$的更佳，也就是前面找到的不可能是最小值。

另一个拉格朗日乘数法重要的在机器学习中的应用，是在SVM中。 