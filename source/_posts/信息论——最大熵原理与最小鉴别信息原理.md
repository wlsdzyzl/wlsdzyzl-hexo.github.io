---
title: 信息论——最大熵原理与最小鉴别信息原理
date: 2019-01-10 00:00:00
tags: [information theory]
categories: 信息论
mathjax: true
---

这一节是信息论的最后一节（当然对于这门学科，这些东西只是冰山一角）。我们聊一聊信息论在压缩啊编码之外的一些作用。  

<!--more-->


假如我们现在面临的是一个传感器网络定位问题，我们希望能够得到传感器网络的节点的位置，但是由于没有一个定位系统，我们只能依靠传感器两两之间的测距信息来估计这个位置。假如有n个结点，而我们的测距信息可能远远达不到结点的个数，这个问题就成为了一个欠定问题。

另外一个问题，比如GPS全球定位原理。对一个点的定位我们需要确定的是4个变量(x,y,z,t)，而天上的卫星个数可能对于4个，使得约束过多，而卫星测距存在噪声，因此可能这些约束并没有一个共同的解，因此这个问题是一个超定问题。对于超定问题实际上是优化问题，可以使用最小二乘法等等方法来解决。

### [](about:blank#%E6%9C%80%E5%A4%A7%E7%86%B5%E5%8E%9F%E7%90%86 "最大熵原理")最大熵原理

面对欠定问题时，方程的可行解对于一个，使得问题变成了一个估计问题，究竟取哪个解才能是最佳的估计。而E.T.Jayne在1957提出的最大熵原理告诉我们，在所有满足约束的解中，能够使得随机变量的熵达到最大，是最佳解。其实本质上是统计方法。

最大熵原理问题表述如下：

设有某一离散随机变量$X$，其概率分布$p(x)$未知，已知其与若干函数的期望：
$$
\sum_{x \in \mathcal{X} } p(x)f_m(x) = C_m,\text{where }m = 1,...,M
$$
求最佳估计$\hat p(x)$

按照最大熵原理求解该问题，可以表述为下面这样的约束优化问题：

*   取概率分布的熵为目标函数$H(X) = -\sum_{x \in \mathcal{X} } p(x)\log p(x)$.
*   约束条件： \sum_{x}p(x)=1\\ \sum_{x}p(x)f_m(x) = C_m,m=1,2,...,M
*   求：$ \hat p(x) = \arg\max _{p(x)} H(X)$

运气很好的是，这个解也是有固定形式的。欠定约束下最大熵分布满足
$$
\hat p(x) = \exp\left[-\lambda_0 - \sum_{m=1}^M \lambda_m f_m(x) \right]
$$
的形式，式中$\lambda_0,…,\lambda_m$的取值使得$\hat p(x)$满足约束条件。

这是一个典型的利用拉格朗日乘子方法求解约束极值的问题。

#### [](about:blank#%E8%BF%9E%E7%BB%AD%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F "连续随机变量")连续随机变量

将离散随机变量推广到连续随机变量，这个问题并没有变得复杂。最大熵原理依然成立，只是将离散熵变成了微分熵。

在连续随机变量的条件下:
$$
p(x)\ge 0\text{ and }p(x)=0 \text{ when }x \in S\\ \int_s p(x)dx = 1\\ \int_s p(x)f_m(x)dx = C_m,m=1,...,M
$$
最大熵原理告诉我们：满足约束条件且使微分熵达到最大值的分布为：
$$
\hat p(x) = \exp\left[\lambda_0 + \sum_{m=1}^M \lambda_mf_m(x)\right].
$$
### [](about:blank#%E6%9C%80%E5%B0%8F%E9%89%B4%E5%88%AB%E4%BF%A1%E6%81%AF%E5%8E%9F%E7%90%86 "最小鉴别信息原理")最小鉴别信息原理

最大熵原理给出的是在没有任何先验信息前提下求解欠定问题的一种方法。如果存在一个先验的分布，如何求解最合理的分布？这时候我们需要用到最小鉴别信息原理。鉴别信息是度量两个分布之间差异的指标，而鉴别信息最小化指的是，在满足约束前提下，给出的分布距离先验分布最小，也就是鉴别信息最小。

最小鉴别信息院里的问题描述如下：

某随机变量$X$，概率分布$q(x)$未知，已知其先验概率密度$p(x)$以及若干函数的期望
$$
\int_s q(x)f_m(x)dx = C_m,m=1,2,...,M
$$
求在上述条件下对$q(x)$的最佳估计。

按照最小鉴别信息原理，上述问题的求解可以表述为以下受限优化问题。

*   取先验分布与目标分布之间的鉴别信息作为目标函数： $$D(q\Vert p) = \int_s q(x)\log\frac{q(x)}{p(x)}dx$$
*   约束条件： $$\int_s q(x)dx = 1\\ \int_s q(x)f_m(x)dx = C_m,m=1,...,M$$
则解为：$\hat q(x) =\arg\min_{q(x)}D(q\Vert p)$。

### [](about:blank#%E4%BA%8C%E8%80%85%E4%B9%8B%E9%97%B4%E8%81%94%E7%B3%BB "二者之间联系")二者之间联系

实际上最大熵原理和最小鉴别信息原理之间联系非常密切。最大熵原理是最小鉴别信息原理的一个特例，而最小鉴别信息原理是最大熵原理的推广。在离散情形下，设先验概率等概：$p(x) = \frac 1 K$，那么二者就是一致的。

证明如下：
$$
\begin{aligned} D(q\Vert p) &= D(q\Vert \frac 1 K)\\ &= \sum_{x \in \mathcal{X} }q(x)\log Kq(x)\\ &= -H(X) + \log K \end{aligned}
$$
因此此时让$D(q \Vert p)$最小就是让$H(X)$最大。

### [](about:blank#Example "Example")Example

*   $S=[0,\infty],EX = \mu$ 最大熵分布为指数分布
*   $S=[-\infty,\infty],EX = \mu$，最大熵分布为方差无穷大的高斯分布
*   $S=[-\infty,\infty],EX = \mu, E(X-\mu)^2 = \sigma^2$，最大熵分布为方差为$\sigma^2$的高斯分布
