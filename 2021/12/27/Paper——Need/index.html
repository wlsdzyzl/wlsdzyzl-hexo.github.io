<!DOCTYPE html>
<html lang="en">
<head>
<script src="https://cdn.staticfile.org/jquery/2.0.0/jquery.min.js">
<script type="text/javascript" src="https://unpkg.com/minigrid@3.1.1/dist/minigrid.min.js"></script>
<link rel="stylesheet" href="/photos/photos.css">
<script type="text/javascript" src="/photos/photos.js"></script>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha256-CTSx/A06dm1B063156EVh15m6Y67pAjZZaQc89LLSrU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"wlsdzyzl.com","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.18.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="这次跟着李沐老师读了Transformer的原始论文：Attention is All You Need。这篇文章不长，但是构架比较新，而且一开始面对的是NLP的问题，所以其实对我来说信息量挺大的；再加上它是后面很多工作的基础，包括attention机制在视觉领域也表现得很好，所以还是专门写一篇记录一下。">
<meta property="og:type" content="article">
<meta property="og:title" content="Paper——Attention is All You Need">
<meta property="og:url" content="http://wlsdzyzl.com/2021/12/27/Paper%E2%80%94%E2%80%94Need/index.html">
<meta property="og:site_name" content="wlsdzyzl">
<meta property="og:description" content="这次跟着李沐老师读了Transformer的原始论文：Attention is All You Need。这篇文章不长，但是构架比较新，而且一开始面对的是NLP的问题，所以其实对我来说信息量挺大的；再加上它是后面很多工作的基础，包括attention机制在视觉领域也表现得很好，所以还是专门写一篇记录一下。">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2021-12-26T22:00:42.000Z">
<meta property="article:modified_time" content="2023-10-22T23:07:35.456Z">
<meta property="article:author" content="wlsdzyzl">
<meta property="article:tag" content="attention">
<meta property="article:tag" content="transformer">
<meta property="article:tag" content="nlp">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://wlsdzyzl.com/2021/12/27/Paper%E2%80%94%E2%80%94Need/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://wlsdzyzl.com/2021/12/27/Paper%E2%80%94%E2%80%94Need/","path":"2021/12/27/Paper——Need/","title":"Paper——Attention is All You Need"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Paper——Attention is All You Need | wlsdzyzl</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">wlsdzyzl</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">無聊時的自娛自樂</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags<span class="badge">89</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories<span class="badge">19</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives<span class="badge">157</span></a></li><li class="menu-item menu-item-photos"><a href="/photos/" rel="section"><i class="photo fa-fw"></i>Photos</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#positional-encoding"><span class="nav-number">1.</span> <span class="nav-text">Positional Encoding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#attention"><span class="nav-number">2.</span> <span class="nav-text">Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#multi-head-attention"><span class="nav-number">2.1.</span> <span class="nav-text">Multi-Head Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mask"><span class="nav-number">2.2.</span> <span class="nav-text">Mask</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B6%E4%BB%96"><span class="nav-number">3.</span> <span class="nav-text">其他</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#position-wise-feed-forward-networks"><span class="nav-number">3.1.</span> <span class="nav-text">Position-wise Feed-Forward
Networks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#layer-normalization"><span class="nav-number">3.2.</span> <span class="nav-text">Layer normalization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">3.3.</span> <span class="nav-text">参考：</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="wlsdzyzl"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">wlsdzyzl</p>
  <div class="site-description" itemprop="description">Everything is choice.</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">157</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">89</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/wlsdzyzl" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;wlsdzyzl" rel="noopener me" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://space.bilibili.com/275872287" title="Bilibili → https:&#x2F;&#x2F;space.bilibili.com&#x2F;275872287" rel="noopener me" target="_blank"><i class="youtube fa-fw"></i>Bilibili</a>
      </span>
  </div>

        </div>
      </div>
    </div>


    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#positional-encoding"><span class="nav-number">1.</span> <span class="nav-text">Positional Encoding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#attention"><span class="nav-number">2.</span> <span class="nav-text">Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#multi-head-attention"><span class="nav-number">2.1.</span> <span class="nav-text">Multi-Head Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mask"><span class="nav-number">2.2.</span> <span class="nav-text">Mask</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B6%E4%BB%96"><span class="nav-number">3.</span> <span class="nav-text">其他</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#position-wise-feed-forward-networks"><span class="nav-number">3.1.</span> <span class="nav-text">Position-wise Feed-Forward
Networks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#layer-normalization"><span class="nav-number">3.2.</span> <span class="nav-text">Layer normalization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">3.3.</span> <span class="nav-text">参考：</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          
<div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Friendly Links</p>
        <div class="site-description" itemprop="description">
        <a target="_blank" rel="noopener" href="https://yakult.fun/" style="color:inherit" >yakult.fun</a>
       </div>
</div>

        </div>
      </div>


    </div>



    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://wlsdzyzl.com/2021/12/27/Paper%E2%80%94%E2%80%94Need/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="wlsdzyzl">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wlsdzyzl">
      <meta itemprop="description" content="Everything is choice.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Paper——Attention is All You Need | wlsdzyzl">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Paper——Attention is All You Need
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-12-27 06:00:42" itemprop="dateCreated datePublished" datetime="2021-12-27T06:00:42+08:00">2021-12-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-10-23 07:07:35" itemprop="dateModified" datetime="2023-10-23T07:07:35+08:00">2023-10-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87/" itemprop="url" rel="index"><span itemprop="name">论文</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>这次跟着李沐老师读了Transformer的原始论文：<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">Attention is All You
Need</a>。这篇文章不长，但是构架比较新，而且一开始面对的是NLP的问题，所以其实对我来说信息量挺大的；再加上它是后面很多工作的基础，包括attention机制在视觉领域也表现得很好，所以还是专门写一篇记录一下。
<span id="more"></span> ### Background</p>
<p>首先，正如题目所说，Transformer第一次是运用在NLP上的，NLP的输入输出是序列，例如从一段中文翻译到一段英文，这种模型被称为<strong>sequence
transduction
models。</strong>处理这样的模型，一般都会使用一个编码解码的架构（Encoder，Decoder）。</p>
<p><strong>Encoder</strong>把输入的符号序列变成一个连续表征(向量)的序列，编码后的序列和原始序列的长度一般是一样的，这个过程可以表示为：</p>
<p><span class="math display">\[
\boldsymbol x = (x_1, \cdots, x_n) \rightarrow \boldsymbol z = (z_1,
\cdots, z_n).
\]</span></p>
<p><strong>Decoder</strong>则把<span class="math inline">\(\boldsymbol
z\)</span>转化为另外一个符号序列，这个符号序列和输入不一定是等长的（例如从中英文翻译一般来说单词数和字数是不等长的）：</p>
<p><span class="math display">\[
\boldsymbol z = (z_1, \cdots, z_n) \rightarrow \boldsymbol y = (y_1,
\cdots, y_m).
\]</span></p>
<p>我们可以认为这些序列是时间序列。这样的序列模型是auto-regressive，也就是会利用过去的值来预测，因此解码器处理当前的内容时不应该看到当前时刻之后的内容。论文中使用一个mask来屏蔽掉当前时刻后的信息。Transformer的整体构架图如下：</p>
<figure>
<img
src="https://evolution-video.oss-cn-beijing.aliyuncs.com/wlsdzyzl_hexo/Attention%20is%20all%20you%20need%20b69fa2a714374c118f25969deef99afb/Untitled.png"
alt="Untitled" />
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p>从构架看出，生成embedding的这个过程在构架图上是直接忽略掉的，直接从inputs到了input
embedding，其实这里的embedding基本是和【<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1608.05859">Using the output embedding to
improve language
models</a>】是一样的，而且两个生成embedding的module权重也是一样的。这幅图上还有很多看不懂的地方，不过我们后面再说（问题：Decoder这里的outputs到底是什么？应该也是symbol序列，似乎是对应的target
sequence）。</p>
<h2 id="positional-encoding">Positional Encoding</h2>
<p>从这里我们应该可以看出来，Attention是一股脑塞进去的，并不会考虑到位置的信息。而单词的前后顺序对于NLP任务来说至关重要，所以文章提出了位置编码：</p>
<p><span class="math display">\[
\begin{aligned}P E_{(p o s, 2 i)} &amp;=\sin \left(\operatorname{pos} /
10000^{2 i / d_{\text {modcl }}}\right) \\P E_{(p o s, 2 i+1)}
&amp;=\cos \left(\operatorname{pos} / 10000^{2 i / d_{\text {model
}}}\right)\end{aligned}
\]</span></p>
<p>具体为何这么设计我也不是太清楚，但是神经网络是黑盒子，只要编码有区分性，就应该能学到东西。文章也提了，使用学习到的位置编码结果与上述编码几乎是一样的。最终单词的embedding就由原来的embedding加上位置编码后的向量得到了。</p>
<h2 id="attention">Attention</h2>
<p>下面我们首先介绍一些文中提到的attention。Attention是一种操作，类似于convolution。说到attention，往往会提到Q（query），K（key）V（value）。这三个是attention的输入，一般来说也就是一组向量，attention也就是根据key来query到value的过程。在文章中，Q，K，V分别是三个矩阵，Q为<span
class="math inline">\(n \times d_k\)</span>, K为<span
class="math inline">\(m \times d_k\)</span>，V为<span
class="math inline">\(m\times d_v\)</span>（这里<span
class="math inline">\(n\)</span>是序列长度，<span
class="math inline">\(m\)</span>是字典的size）。当拿到<span
class="math inline">\(Q\)</span>矩阵后，我们根据其中的向量和所有的key做相似度的计算，并且根据这个相似度去计算weights，这样Q中每一个向量最后就对应了一个value的加权和。本文中相似度的计算直接使用的是向量的点乘，最终weights是将相似度输入到softmax中得到的，因此上述步骤很容易写成矩阵乘法的形式：</p>
<p><span class="math display">\[
\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q
K^{T}}{\sqrt{d_{k}}}\right) V
\]</span></p>
<p>可以看到<span class="math inline">\(QK^T\)</span>得到了一个<span
class="math inline">\(n \times
m\)</span>矩阵，每一行是Q中的每个向量到<span
class="math inline">\(m\)</span>个key的相似度，接着经过softmax层，会输出<span
class="math inline">\(n \times
m\)</span>的矩阵。因为softmax的输出天然是合为1，可以直接当作权重来使用。这里<span
class="math inline">\(\frac{1}{\sqrt {d_k}}\)</span>是一个scaling
factor，主要是因为当维度<span
class="math inline">\(d_k\)</span>很大时，点乘的结果也会很大，就会把softmax函数推到梯度很小的区域。因此这里使用一个scaling
factor来抵消这种影响。这个过程如下：</p>
<figure>
<img
src="https://evolution-video.oss-cn-beijing.aliyuncs.com/wlsdzyzl_hexo/Attention%20is%20all%20you%20need%20b69fa2a714374c118f25969deef99afb/Untitled%201.png"
alt="Untitled" />
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<h3 id="multi-head-attention">Multi-Head Attention</h3>
<p>原始的Attention相比于Convolution的一个缺点是，convolution可以输出多个channel（也就需要多个卷积核）。而Attention本身并没有什么需要学习的参数，复制多个似乎并没有什么意义。文中提出了Multi-head
Attention来达到这种效果。她的方法就是使用多个投影后的Q，K以及V，来同时进行Attention操作。具体做法如下：</p>
<p><span class="math display">\[
\begin{aligned}\operatorname{MultiHead}(Q, K, V) &amp;=\text { Concat
}\left(\operatorname{head}_{1}, \ldots, \text { head
}_{\mathrm{h}}\right) W^{O} \\\text { where head }_{\mathrm{i}}
&amp;=\text { Attention }\left(Q W_{i}^{Q}, K W_{i}^{K}, V
W_{i}^{V}\right)\end{aligned}
\]</span></p>
<figure>
<img
src="https://evolution-video.oss-cn-beijing.aliyuncs.com/wlsdzyzl_hexo/Attention%20is%20all%20you%20need%20b69fa2a714374c118f25969deef99afb/Untitled%202.png"
alt="Untitled" />
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p>这里<span class="math inline">\(W_{i}^{Q} \in \mathbb{R}^{d_{\text
{model }} \times d_{k}}, W_{i}^{K} \in \mathbb{R}^{d_{\text {model }}
\times d_{k}}, W_{i}^{V} \in \mathbb{R}^{d_{\text {model }} \times
d_{v}}\)</span>以及<span class="math inline">\(W^{O} \in \mathbb{R}^{h
d_{v} \times d_{\text {model
}}}\)</span>。使用线性投影的好处就是使得输入输出的维度更加灵活。</p>
<h3 id="mask">Mask</h3>
<p>Mask是一个和输入维度相同的binary矩阵，用来屏蔽掉一些输入。一般来说，有两个地方是需要使用mask的。</p>
<p>第一个是再处理的时候，会因为sequence往往是不等长的，一般的做法是使用某个默认的向量例如0来填充这些空缺，使得维度上是相等的。但是在真正处理时，我们是希望这些空缺是不起作用的，所以就会使用到mask。</p>
<figure>
<img
src="https://evolution-video.oss-cn-beijing.aliyuncs.com/wlsdzyzl_hexo/Attention%20is%20all%20you%20need%20b69fa2a714374c118f25969deef99afb/Untitled.jpeg"
alt="Untitled" />
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p>第二个情况，就是之前提到的，我们的模型是augoregressive的，每次给decoder处理时，是一个单词一个单词进行处理，而当前处理的内容不应该看到之后的内容。但是为了并行，我们是一起放进去的，所以需要一个mask来屏蔽掉之后的东西。一般来说，这样的mask是个下三角矩阵，这样对第一行，就屏蔽掉了第一个之后的内容，第二行就屏蔽掉了第二个embedding之后的内容。</p>
<blockquote>
<p><em>Below the attention mask shows the position each tgt word (row)
is allowed to look at (column). Words are blocked for attending to
future words during training.</em></p>
</blockquote>
<figure>
<img
src="https://evolution-video.oss-cn-beijing.aliyuncs.com/wlsdzyzl_hexo/Attention%20is%20all%20you%20need%20b69fa2a714374c118f25969deef99afb/Untitled%203.png"
alt="Untitled" />
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p>具体方法如何屏蔽呢？因为后面跟得层是softmax层，所以置零是不可取的，一般来说会置一个很大的负数，例如-1e9，这样经过softmax后的权重就等于0了。这个mask是如何使用的？具体来说，经过对target进行embedding后，我们得到了序列<span
class="math inline">\(n\times d_{model}\)</span>的矩阵（<span
class="math inline">\(n\)</span>为target
sequence的长度），如果一个单词一个单词处理的话，当前的单词可以看到之前的单词，但是不能看到之后的单词，如下图：</p>
<figure>
<img
src="https://evolution-video.oss-cn-beijing.aliyuncs.com/wlsdzyzl_hexo/Attention%20is%20all%20you%20need%20b69fa2a714374c118f25969deef99afb/Untitled%204.png"
alt="Untitled" />
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p>而实际中，我们会做成一个<span class="math inline">\(n \times n \times
d_{model}\)</span>的tensor，可以并行处理，因此就需要使用上述的sequence
mask来去掉之后的内容。</p>
<h2 id="其他">其他</h2>
<h3 id="position-wise-feed-forward-networks">Position-wise Feed-Forward
Networks</h3>
<p>其实就是MLP：</p>
<p><span class="math display">\[
\operatorname{FFN}(x)=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2}
\]</span></p>
<h3 id="layer-normalization">Layer normalization</h3>
<p>Layer normalization和batch normalization是不一样的。例如我们设定batch
size为<span class="math inline">\(m\)</span>，feature dimension为<span
class="math inline">\(n\)</span>，那么我们一个batch可以表示为<span
class="math inline">\(m \times n\)</span>的矩阵。batch
normalization是看成n个样本来做normalization（每一行为一个样本），而layer
normalization是看成m个样本来做normalization（每一列为一个样本）。</p>
<p>需要注意的是，在本文的NLP处理中，由于输入的是sequence，所以得到的并不是矩阵，而是tensor（<span
class="math inline">\(m \times s \times n\)</span> ）：</p>
<figure>
<img
src="https://evolution-video.oss-cn-beijing.aliyuncs.com/wlsdzyzl_hexo/Attention%20is%20all%20you%20need%20b69fa2a714374c118f25969deef99afb/Untitled%201.jpeg"
alt="Untitled" />
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p>所以对应的样本的“切割”方式也不一样了。至于为什么使用layer
normalization，下面一张图也许可以解释一下：</p>
<figure>
<img
src="https://evolution-video.oss-cn-beijing.aliyuncs.com/wlsdzyzl_hexo/Attention%20is%20all%20you%20need%20b69fa2a714374c118f25969deef99afb/Untitled%202.jpeg"
alt="Untitled" />
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p>因为sequence经常是不等长的（有效部分使用阴影标出），所以很明显黄色线条看作为样本再来进行normalization是更合适的方式。</p>
<p>这篇文章就先介绍到这里。NLP并不是我研究的方向，但是Transformer在视觉上也表现很强，所以学习一下，后面会更新在视觉方面的改进和应用。</p>
<h3 id="参考">参考：</h3>
<p><a
target="_blank" rel="noopener" href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">https://nlp.seas.harvard.edu/2018/04/03/attention.html</a></p>
<p><a
target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1pu411o7BE?from=search&amp;seid=11778794199464387051&amp;spm_id_from=333.337.0.0">https://www.bilibili.com/video/BV1pu411o7BE</a></p>
<p><a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/338817680">https://zhuanlan.zhihu.com/p/338817680</a></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/attention/" rel="tag"># attention</a>
              <a href="/tags/transformer/" rel="tag"># transformer</a>
              <a href="/tags/nlp/" rel="tag"># nlp</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/12/17/Nets%EF%BC%88GAN%EF%BC%89/" rel="prev" title="Paper——Generative Adversarial Nets（GAN）">
                  <i class="fa fa-angle-left"></i> Paper——Generative Adversarial Nets（GAN）
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/12/17/Paper_GraphNetwork/" rel="next" title="图神经网络（Graph Neural Network）">
                  图神经网络（Graph Neural Network） <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2023</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">wlsdzyzl</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>




</body>
</html>
