<!DOCTYPE html>
<html lang="en">
<head>
<script src="https://cdn.staticfile.org/jquery/2.0.0/jquery.min.js">
<script type="text/javascript" src="https://unpkg.com/minigrid@3.1.1/dist/minigrid.min.js"></script>
<link rel="stylesheet" href="/photos/photos.css">
<script type="text/javascript" src="/photos/photos.js"></script>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha256-CTSx/A06dm1B063156EVh15m6Y67pAjZZaQc89LLSrU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"wlsdzyzl.com","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.18.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="这周上的数据学习，主要讲了一些神经网络相关的知识。神经网络是目前最流行的机器学习算法了，甚至由它诞生了一个新的学科：deep learning。因此一篇博客，只能浅浅介绍一些神经网络的基本内容。">
<meta property="og:type" content="article">
<meta property="og:title" content="Learning From Data——Neural Network">
<meta property="og:url" content="http://wlsdzyzl.com/2018/11/06/Learning-From-Data%E2%80%94%E2%80%94Neural-Network/index.html">
<meta property="og:site_name" content="wlsdzyzl">
<meta property="og:description" content="这周上的数据学习，主要讲了一些神经网络相关的知识。神经网络是目前最流行的机器学习算法了，甚至由它诞生了一个新的学科：deep learning。因此一篇博客，只能浅浅介绍一些神经网络的基本内容。">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2018-11-06T05:54:51.000Z">
<meta property="article:modified_time" content="2023-10-20T19:34:00.408Z">
<meta property="article:author" content="wlsdzyzl">
<meta property="article:tag" content="machine learning">
<meta property="article:tag" content="LFD class">
<meta property="article:tag" content="deep learning">
<meta property="article:tag" content="neural network">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://wlsdzyzl.com/2018/11/06/Learning-From-Data%E2%80%94%E2%80%94Neural-Network/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://wlsdzyzl.com/2018/11/06/Learning-From-Data%E2%80%94%E2%80%94Neural-Network/","path":"2018/11/06/Learning-From-Data——Neural-Network/","title":"Learning From Data——Neural Network"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Learning From Data——Neural Network | wlsdzyzl</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">wlsdzyzl</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">無聊時的自娛自樂</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags<span class="badge">89</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories<span class="badge">19</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives<span class="badge">157</span></a></li><li class="menu-item menu-item-photos"><a href="/photos/" rel="section"><i class="photo fa-fw"></i>Photos</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%ADforward-propagation"><span class="nav-number">1.</span> <span class="nav-text">前向传播（forward
propagation）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cost-function"><span class="nav-number">2.</span> <span class="nav-text">cost function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E5%8C%96vectorize"><span class="nav-number">3.</span> <span class="nav-text">向量化（vectorize）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%8E%E5%90%91%E4%BC%A0%E6%92%ADback-propagation"><span class="nav-number">4.</span> <span class="nav-text">后向传播（back propagation）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96parameter-initialization"><span class="nav-number">4.1.</span> <span class="nav-text">参数初始化（Parameter
Initialization）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6gradient"><span class="nav-number">4.2.</span> <span class="nav-text">梯度（gradient）</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="wlsdzyzl"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">wlsdzyzl</p>
  <div class="site-description" itemprop="description">Everything is choice.</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">157</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">89</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/wlsdzyzl" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;wlsdzyzl" rel="noopener me" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://space.bilibili.com/275872287" title="Bilibili → https:&#x2F;&#x2F;space.bilibili.com&#x2F;275872287" rel="noopener me" target="_blank"><i class="youtube fa-fw"></i>Bilibili</a>
      </span>
  </div>

        </div>
      </div>
    </div>


    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%ADforward-propagation"><span class="nav-number">1.</span> <span class="nav-text">前向传播（forward
propagation）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cost-function"><span class="nav-number">2.</span> <span class="nav-text">cost function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E5%8C%96vectorize"><span class="nav-number">3.</span> <span class="nav-text">向量化（vectorize）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%8E%E5%90%91%E4%BC%A0%E6%92%ADback-propagation"><span class="nav-number">4.</span> <span class="nav-text">后向传播（back propagation）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96parameter-initialization"><span class="nav-number">4.1.</span> <span class="nav-text">参数初始化（Parameter
Initialization）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6gradient"><span class="nav-number">4.2.</span> <span class="nav-text">梯度（gradient）</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          
<div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Friendly Links</p>
        <div class="site-description" itemprop="description">
        <a target="_blank" rel="noopener" href="https://yakult.fun/" style="color:inherit" >yakult.fun</a>
       </div>
</div>

        </div>
      </div>


    </div>



    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://wlsdzyzl.com/2018/11/06/Learning-From-Data%E2%80%94%E2%80%94Neural-Network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="wlsdzyzl">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wlsdzyzl">
      <meta itemprop="description" content="Everything is choice.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Learning From Data——Neural Network | wlsdzyzl">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Learning From Data——Neural Network
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2018-11-06 13:54:51" itemprop="dateCreated datePublished" datetime="2018-11-06T13:54:51+08:00">2018-11-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-10-21 03:34:00" itemprop="dateModified" datetime="2023-10-21T03:34:00+08:00">2023-10-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">数据学习课程</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>这周上的数据学习，主要讲了一些神经网络相关的知识。神经网络是目前最流行的机器学习算法了，甚至由它诞生了一个新的学科：deep
learning。因此一篇博客，只能浅浅介绍一些神经网络的基本内容。 <span id="more"></span>
据说神经网络制造出来是为了模拟大脑。不过我认为离这个目标还差的远。但是呢，Neural
Network确实做出来一些很牛逼的事情，让它成为现在AI中最受欢迎的技术。不过神经网络的概念倒是很早很早就提出了，之前没落是因为计算的性能跟不上。现在又东山再起了。</p>
<p>即使没有接触过机器学习，也一定听过神经网络学习，以及见过类似下面的图：</p>
<p><img
src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Artificial_neural_network.svg/560px-Artificial_neural_network.svg.png" /></p>
<p>实际上，这就是神经网络。神经网络的原理不复杂，但是如果嵌套层数比较多，就会需要非常大的计算量。</p>
<p>对于每一个神经元，我们都可以把它看作之前的一个logistic
regression。每一个神经元可以接受输入，然后提供输出，要么作为最终的输出，要么给别的神经元提供输入。</p>
<h2 id="前向传播forward-propagation">前向传播（forward
propagation）</h2>
<p>这意味着，我们的<span
class="math inline">\(W\)</span>参数将会变成一个”张量”（这么说也许不够准确，因为它不第一层可能有5个神经元，第2层可能有4个，也就是不是一个立方体）。在这里，我们用<span
class="math inline">\(\Theta\)</span>来表示这个张量。<span
class="math inline">\(\Theta^{(i)}\)</span>表示第i层的<span
class="math inline">\(\theta\)</span>参数，而<span
class="math inline">\(\Theta^{(i)}_j\)</span>表示第i层，第j个theta向量，<span
class="math inline">\(Theta^{(i)}_{j,k}\)</span>表示的就是某个具体的参数值了。</p>
<p>我们使用<span
class="math inline">\(a^{(i)}_j\)</span>按照上面的规则来表示第i层第j个神经元的输出。</p>
<p>所以，就上面的这个图，我们可以很容易得出： <span
class="math display">\[
a_1^{(1)} = g(\Theta_{1,0}^{(1)}x_0 + \Theta_{1,1}^{(1)}x_1 +
\Theta_{1,2}^{(2)}x_2+\Theta_{1,3}^{(3)}x_3 )\\
a_2^{(1)} = g(\Theta_{2,0}^{(1)}x_0 + \Theta_{2,1}^{(1)}x_1 +
\Theta_{2,2}^{(2)}x_2+\Theta_{2,3}^{(3)}x_3 )\\
a_3^{(1)} = g(\Theta_{3,0}^{(1)}x_0 + \Theta_{3,1}^{(1)}x_1 +
\Theta_{3,2}^{(2)}x_2+\Theta_{3,3}^{(3)}x_3 )
a_4^{(1)} = g(\Theta_{4,0}^{(1)}x_0 + \Theta_{4,1}^{(1)}x_1 +
\Theta_{4,2}^{(2)}x_2+\Theta_{4,3}^{(3)}x_3 )\\
h_{\Theta}(X) =\\
\begin{bmatrix}
g(\Theta_{1,0}^{(2)}a_0^{(1)} + \Theta_{1,1}^{(2)}a_1^{(1)} +
\Theta_{1,2}^{(2)}a_2^{(1)}
+\Theta_{1,3}^{(2)}a_3^{(1)}+\Theta_{1,4}^{(2)}a_4^{(1)} )\\
g(\Theta_{2,0}^{(2)}a_0^{(1)} + \Theta_{2,1}^{(2)}a_1^{(1)} +
\Theta_{2,2}^{(2)}a_2^{(1)}
+\Theta_{2,3}^{(2)}a_3^{(1)}+\Theta_{2,4}^{(2)}a_4^{(1)} )
\end{bmatrix}\\
= [a_1^{(2)},a_2^{(2)} ]^T
\]</span></p>
<p>上面的神经网络输出有两项。</p>
<p>上面的函数中，g为logistic函数，又叫sigmoid函数。当然这个函数不仅仅局限于sigmoid函数，也有relu函数，tanh函数：
<span class="math display">\[
\begin{matrix}
g(z) = \frac 1 {1+e^{-z} } &amp;(sigmoid)\\
g(z) = \max(z,0) &amp;(ReLU)\\
g(z) = \frac{e^z - e^{-z} }{e^z + e^{-z} }&amp; (tanh)
\end{matrix}
\]</span></p>
<p>我们定义<span class="math inline">\(a_{j}^1\)</span>为原始输入。</p>
<p>我们可以使用向量化来加速神经网络的计算过程。这个应该不算很稀奇的技术。对于每一层来说，<span
class="math inline">\(theta\)</span>都是严格的矩阵的,而输入也是一个矩阵。所以每一层的向量化都不算困难。</p>
<p>通过输入，计算出每一层的输出，然后将这层输出当作下一层的输入，最后得到最后的结果，这就是前向传播。前向传播实际上就是神经网络怎么计算出结果的过程。</p>
<p>利用神经网络我们可以很容易地实现很复杂的非线性函数的边界，从而进行分类。吴恩达在视频中介绍了用神经网络对与，或非以及异或的实现。不过这些都是相对简单的，复杂的神经网络分析其来完全没有这么容易，这也是神经网络强大的一个原因。</p>
<p>为了实现多个分类，我们可以将最后的输出定位k个，分别来判断是否为当前类。这个做法实际上one-Vs-all的做法。</p>
<h2 id="cost-function">cost function</h2>
<p>接下来我们来谈谈neural network的cost function。之前的logistic
regression的cost function是通过计算极大似然估计得到的。而神经网络的cost
funtion实际上呢也是一样的，不过它的h(x)不再是之前那么简单了，这里的<span
class="math inline">\(W\)</span>变成了<span
class="math inline">\(\Theta\)</span>。而且由于输出可能是多元的（多元分类），所以这个cost
funtion实际上是各个类别的cost funtion的叠加： <span
class="math display">\[
J(\Theta) = -\frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K y_i^{(k)}
\log(h_{\Theta}(X_i))_k + (1-y_i^{(k)}) \log (1 - (h_\Theta(X_i))_k)
\]</span></p>
<p>如果加上正则化的话，则正则化项为：<span class="math inline">\(\frac
\lambda {2m} \sum_{l=1}^{L}\sum_{i=1}^{s_{l} }\sum_{j=1}{s_{l-1} }
(\Theta_{i,j}^{(l)})^2\)</span></p>
<p>需要注意的是这个<span
class="math inline">\(s_{l+1}\)</span>,为何是这样？<span
class="math inline">\(s_l\)</span>表示了第l层有多少神经元（如果l=0则表示有多少个原始输入），也就表示了有多少个输入向量，而输入向量的长度则由前一个输入的个数决定，因此<span
class="math inline">\(s_{l-1}\)</span>实际上输入向量的长度。所以呢，我们先从层数开始，然后到该层的每个神经元，最后再到每个参数的每个取值。
（这里和吴恩达的课程的表述稍微有点区别，也就是我的输入是表示为第0层，也就是一共有L+1层，而吴恩达的课程中，共有L层，原来的0变成现在的第1层。不过这意味着，从1开始计数，由于第一层是没有theta参数的，所以第一层计算的实际是下一层的theta，而<span
class="math inline">\(s_l\)</span>是下一层的输入向量长度，而<span
class="math inline">\(s_{l+1}\)</span>才是输入向量的个数，至于最后一层的theta由于已经在前一次计算过了，所以这里的regularization为<span
class="math inline">\(\frac \lambda {2m}
\sum_{l=1}^{L-1}\sum_{i=1}^{s_{l} }\sum_{j=1}{s_{l+1} }
(\Theta_{j,i}^{(l)})^2\)</span>）.</p>
<h2 id="向量化vectorize">向量化（vectorize）</h2>
<p>首先，为了方便后面的说明，我们定义</p>
<p><span class="math inline">\(z_{j}^{(i)} =
(\Theta_{j}^{(i)})^Ta^{(i-1)}\)</span>.</p>
<p>而<span class="math inline">\(a_{j}^{(i)} =
g(z_{j}^{(i)})\)</span>.</p>
<p><span class="math inline">\(a^{(i-1)} =
[a_0^{(i-1)},a_1^{(i-1)},...,a_{s_{i-1} }^{(i-1)} ]\)</span></p>
<p>希望大家还没有忘记这些符号以及下标的意义。</p>
<p>如果一个符号，只有层数的上标，没有下标，则意味着它是一个向量(etc.<span
class="math inline">\(a^{(i)},z^{(i)}\)</span>)，或是一个矩阵<span
class="math inline">\(\Theta^{(i)}\)</span>.</p>
<p>通过向量化，我们可以像下面一样通过线性代数库的并行优化，很快的计算出来<span
class="math inline">\(z^{(i)}\)</span>：</p>
<p><span class="math display">\[
z^{(i)} = \Theta^{(i)} a^{(i-1)}
\]</span></p>
<p>上式中： <span class="math display">\[
\Theta^{(i)}  = \begin{bmatrix}
...  (\Theta_{1}^{(i)})^T ... \\
...  (\Theta_{2}^{(i)})^T ... \\
...\\
  ...  (\Theta_{s_i}^{(i)})^T ...
\end{bmatrix}
\]</span></p>
<p>从这里，我们也可以知道了，为什么g不用identify function（g(z) =
z）.因为神经网络的提出，是为了进行非线性的分类和预测。而： <span
class="math display">\[
\begin{aligned}
z^{(i)} &amp;= \Theta^{(i)} a^{(i-1)}\\
&amp;=\Theta^{(i)}  g(z^{(i-1)})\\
&amp;=\Theta^{(i)}  z^{(i-1)}\\
&amp;= \Theta^{(i)}  \Theta ^{(i-1)} z^{(i-2)}\\
&amp;= \Theta^{(i)} \Theta^{(i-1)}...\Theta^{(1)} X_1
\end{aligned}
\]</span></p>
<p>这意味着我们通过线性的函数来做神经网络是无法得到非线性的分类结果的。</p>
<p>如果我们再对训练样本利用向量化， <span class="math display">\[
Z^{(l)} = \Theta^{(l)} ( A^{(l-1)})^T
\]</span> 这时候呢，Z变成矩阵了(<span class="math inline">\(s_{l} \times
m)\)</span>)，A也变成矩阵了（$ m s_{l-1} $）。而Z^{(l)}实际如下：</p>
<p><span class="math display">\[
Z^{(l)} =\begin{bmatrix}
\vert &amp; ... &amp; \vert\\
z^{(l)[1 ]} &amp; ... &amp;z^{(l)[m ]}\\
\vert &amp; ... &amp; \vert
\end{bmatrix}
\]</span></p>
<h2 id="后向传播back-propagation">后向传播（back propagation）</h2>
<h3 id="参数初始化parameter-initialization">参数初始化（Parameter
Initialization）</h3>
<p>需要注意的是，神经网络中我们不能简单地将参数初始化为0.经过计算你就会明白，如果参数初始化为0,则计算出来的梯度就是一样的，无法进行梯度下降,无论怎么运行，最后得到的结果为<span
class="math inline">\(0.5\)</span>(sigmoid)。可以进行随机初始化，给每个参数一个很小的值<span
class="math inline">\(N(0,0.1)\)</span>.</p>
<p>在实际中，证明了有比随机初始化更好的方法来进行初始化：Xavier/He
initialization.</p>
<p><span class="math display">\[
\Theta ^{(l)} \tilde{} N\left(0,\sqrt{\frac {2}{s_l + s_{l-1} } }
\right)
\]</span></p>
<h3 id="梯度gradient">梯度（gradient）</h3>
<p>后向传播实际上是建立在梯度下降的基础上的，所以最复杂的部分就是计算梯度了。</p>
<p>假设，这个层数一共有L层，最后的输出是<span
class="math inline">\(a^{L}\)</span>，因此cost
funtion和L层的参数是直接相关的。所以首先计算的就是cost funtion对<span
class="math inline">\(\Theta^{(L)}\)</span>的梯度。</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial L(\Theta)}{ \partial \Theta^{(L)} } &amp;=
-\frac{\partial}{\partial \Theta^{(L)} } \left((1-y)\log (1 - y&#39;)  +
y\log y&#39;\right)\\
&amp;=-(1 - y)\frac{\partial}{\partial \Theta^{(L)} } \log (1 -
g(\Theta^{(L)}a^{(L-1)}))  - y \frac{\partial}{\partial \Theta^{(L)} }
\log g(\Theta^{(L)}a^{(L-1)})\\
&amp;= (1-y)\frac{1}{1 - g(\Theta^{(L)}a^{(L-1)})}
g&#39;(\Theta^{(L)}a^{(L-1)}))(a^{(L-1)})^T - y \frac
{1}{g(\Theta^{(L)}a^{(L-1)} }
g&#39;(\Theta^{(L)}a^{(L-1)}(a^{(L-1)})^T\\
&amp;= (1-y)\sigma(\Theta^{(L)}a^{(L-1)})(a^{(L-1)})^T - y(1 -
\sigma(\Theta^{(L)}a^{(L-1)})) (a^{(L-1)})^T\\
&amp;= (1 - y)a^{(L)} (a^{(L-1)})^T - y (1 - a^{(L)})(a^{(L-1)})^T\\
&amp;= (a^{(L)} - y)(a^{(L-1)})^T
\end{aligned}
\]</span></p>
<p>上述推导过程中，g为sigmoid函数，因此<span
class="math inline">\(g&#39; = \sigma&#39; = \sigma(1 -
\sigma)\)</span>.</p>
<p><span
class="math inline">\(a^{(i)}\)</span>我们都是可以通过前向传播得到的。</p>
<p>然后我们想要计算的是<span
class="math inline">\(\Theta{(L-1)},...,\Theta{(1)}\)</span>这些的梯度。但是它们是和<span
class="math inline">\(L(\Theta)\)</span>是没有直接的关系的。不过，在微积分中是有链式求导法则的：</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial L}{\partial \Theta^{(L-1)} } &amp;= \frac{\partial
L}{a^{(L)} } \frac{a^{(L)} }{\partial \Theta^{(L-1)} }\\
&amp;=\frac{\partial L}{a^{(L)} } \frac{a^{(L)} }{z^{(L)} }
\frac{z^{(L)} }{\Theta^{(L-1)} }\\
&amp;=\frac{\partial L}{a^{(L)} } \frac{a^{(L)} }{z^{(L)} }
\frac{z^{(L)} }{a^{(L-1)} }\frac{a^{(L-1)} }{\Theta^{(L-1)} }\\
&amp;=\underbrace{\frac{\partial L}{a^{(L)} } \frac{a^{(L)} }{z^{(L)} }
}_{a^{(L)} - y}\underbrace{ \frac{z^{(L)} }{a^{(L-1)} } }_{\Theta^{(L)}
}\underbrace{\frac{a^{(L-1)} }{z^{(L-1)} }
}_{g&#39;(z^{(L-1)})}\underbrace{\frac{z^{(L-1)} }{\Theta^{(L-1)} }
}_{a^{(L-2)} }
\end{aligned}
\]</span></p>
<p>这其中，一个个的导数都是可以计算出来的。因此我们就得到了最终的梯度：
<span class="math display">\[
\begin{aligned}
\frac{\partial L}{a^{(L)} } \frac{a^{(L)} }{z^{(L)} } \frac{z^{(L)}
}{a^{(L-1)} }\frac{a^{(L-1)} }{z^{(L-1)} }\frac{z^{(L-1)}
}{\Theta^{(L-1)} } = \underbrace{(a^{(L)} -
y)}_{s_L\times1}\underbrace{\Theta^{(L)} }_{s_l \times s_{l-1}
}\underbrace{g&#39;(z^{(L-1)})}_{s_{l-1} \times 1}\underbrace{a^{(L-2)}
}_{s_{l-2} \times 1}
\end{aligned}
\]</span></p>
<p>不过需要注意的是，上面得到的导数你会发现矩阵的维度可能不合适。因此这个形式必须要重新组织一下：
<span class="math display">\[
\underbrace{\frac{\partial L}{\partial \Theta^{(L-1)} } }_{s_{l-1}
\times s_{l-2} }=\underbrace{(\Theta^{(L)})^T }_{s_{l-1} \times s_{l}
}\underbrace{(a^{(L)}-y)}_{s_l \times 1} .*
\underbrace{g&#39;(z^{(L-1)})}_{s_{l-1}\times
1}\underbrace{(a^{(L-2)})^T}_{1 \times s_{l-2} }
\]</span></p>
<p>如果想要计算更前面的参数矩阵的导数，这个链式法则会越来越长。</p>
<p>为了更好的计算各个层的梯度，我们新定义一个符号: <span
class="math display">\[
\sigma^{(l)} = \nabla_{z^{(l)} }L(y,y&#39;)
\]</span></p>
<ul>
<li><strong><span class="math inline">\(l = L\)</span></strong></li>
</ul>
<p>有时候我们可以直接计算出来<span class="math inline">\(\nabla_{z^{(L)}
}L(y,y&#39;)\)</span>(g为softmax函数)，有时候需要使用链式法则:</p>
<p><span class="math inline">\(\nabla_{z^{(L)} }L(y,y&#39;) =
\nabla_{y&#39;}L(y&#39;,y) .* g&#39;(z^{(L)})\)</span></p>
<ul>
<li><strong><span class="math inline">\(l \ne L\)</span></strong></li>
</ul>
<p><span class="math inline">\(\sigma^{l} =
((\Theta^{(l+1))^T}\sigma^{(l+1)}) .*g&#39;(z^{(l)})\)</span></p>
<ul>
<li><span class="math inline">\(\nabla _{\Theta^{(l)} } L =
\sigma^{(l)}(a^{(l-1)})^T\)</span></li>
</ul>
<p>通过验证你会发现实际上上面说的正是我们推导的内容。</p>
<p>使用梯度下降,或者SGD（更加常用），最终求得合适的<span
class="math inline">\(\Theta\)</span>.</p>
<p>可以看到的<span
class="math inline">\(\sigma^{(l)}\)</span>的计算，需要的是后向计算，所以这个叫后向传播。</p>
<p>上面的推导过程都是以一个training
example的，对于多个样本可以通过向量化以及矩阵化来加快实现。</p>
<p>参考文献：</p>
<p><a
target="_blank" rel="noopener" href="http://cs229.stanford.edu/notes/cs229-notes-deep_learning.pdf">css229:deep
learning</a></p>
<p><a
target="_blank" rel="noopener" href="http://cs229.stanford.edu/notes/cs229-notes-backprop.pdf">css229:back-prop</a></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/machine-learning/" rel="tag"># machine learning</a>
              <a href="/tags/LFD-class/" rel="tag"># LFD class</a>
              <a href="/tags/deep-learning/" rel="tag"># deep learning</a>
              <a href="/tags/neural-network/" rel="tag"># neural network</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2018/11/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Kernel-Logistic-Regression/" rel="prev" title="机器学习——Kernel Logistic Regression">
                  <i class="fa fa-angle-left"></i> 机器学习——Kernel Logistic Regression
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2018/11/06/Learning-From-Data%E2%80%94%E2%80%94Covariance-Matrix-Derivation/" rel="next" title="Learning From Data——Covariance Matrix Derivation">
                  Learning From Data——Covariance Matrix Derivation <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2023</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">wlsdzyzl</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>




</body>
</html>
