<!DOCTYPE html>
<html lang="en">
<head>
<script src="https://cdn.staticfile.org/jquery/2.0.0/jquery.min.js">
<script type="text/javascript" src="https://unpkg.com/minigrid@3.1.1/dist/minigrid.min.js"></script>
<link rel="stylesheet" href="/photos/photos.css">
<script type="text/javascript" src="/photos/photos.js"></script>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha256-CTSx/A06dm1B063156EVh15m6Y67pAjZZaQc89LLSrU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"wlsdzyzl.com","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.18.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="总共20道题目。">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习——（基石）作业3">
<meta property="og:url" content="http://wlsdzyzl.com/2018/09/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%EF%BC%88%E5%9F%BA%E7%9F%B3%EF%BC%89%E4%BD%9C%E4%B8%9A3/index.html">
<meta property="og:site_name" content="wlsdzyzl">
<meta property="og:description" content="总共20道题目。">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2018-09-05T13:33:40.000Z">
<meta property="article:modified_time" content="2023-10-20T20:21:55.038Z">
<meta property="article:author" content="wlsdzyzl">
<meta property="article:tag" content="machine learning">
<meta property="article:tag" content="homework">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://wlsdzyzl.com/2018/09/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%EF%BC%88%E5%9F%BA%E7%9F%B3%EF%BC%89%E4%BD%9C%E4%B8%9A3/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://wlsdzyzl.com/2018/09/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%EF%BC%88%E5%9F%BA%E7%9F%B3%EF%BC%89%E4%BD%9C%E4%B8%9A3/","path":"2018/09/05/机器学习——（基石）作业3/","title":"机器学习——（基石）作业3"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>机器学习——（基石）作业3 | wlsdzyzl</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">wlsdzyzl</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">無聊時的自娛自樂</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags<span class="badge">89</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories<span class="badge">19</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives<span class="badge">156</span></a></li><li class="menu-item menu-item-photos"><a href="/photos/" rel="section"><i class="photo fa-fw"></i>Photos</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#p.s."><span class="nav-number">1.</span> <span class="nav-text">p.s.</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="wlsdzyzl"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">wlsdzyzl</p>
  <div class="site-description" itemprop="description">Everything is choice.</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">156</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">89</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/wlsdzyzl" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;wlsdzyzl" rel="noopener me" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://space.bilibili.com/275872287" title="Bilibili → https:&#x2F;&#x2F;space.bilibili.com&#x2F;275872287" rel="noopener me" target="_blank"><i class="youtube fa-fw"></i>Bilibili</a>
      </span>
  </div>

        </div>
      </div>
    </div>


    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#p.s."><span class="nav-number">1.</span> <span class="nav-text">p.s.</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          
<div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Friendly Links</p>
        <div class="site-description" itemprop="description">
        <a target="_blank" rel="noopener" href="https://yakult.fun/" style="color:inherit" >yakult.fun</a>
       </div>
</div>

        </div>
      </div>


    </div>



    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://wlsdzyzl.com/2018/09/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%EF%BC%88%E5%9F%BA%E7%9F%B3%EF%BC%89%E4%BD%9C%E4%B8%9A3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="wlsdzyzl">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wlsdzyzl">
      <meta itemprop="description" content="Everything is choice.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="机器学习——（基石）作业3 | wlsdzyzl">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习——（基石）作业3
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2018-09-05 21:33:40" itemprop="dateCreated datePublished" datetime="2018-09-05T21:33:40+08:00">2018-09-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-10-21 04:21:55" itemprop="dateModified" datetime="2023-10-21T04:21:55+08:00">2023-10-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>总共20道题目。</p>
<span id="more"></span>
<p><strong>1. Consider a noisy target <span class="math inline">\(y =
{\bf w}_f^T{\bf x} + \epsilon\)</span>, where <span
class="math inline">\(\mathbf{x} \in \mathbb{R}^d\)</span> (with the
added coordinate <span class="math inline">\(x_0=1\)</span>), <span
class="math inline">\(y\in\mathbb{R}\)</span>, <span
class="math inline">\(\mathbf{w}_f\)</span> is an unknown vector, and
<span class="math inline">\(\epsilon\)</span> is a noise term with zero
mean and <span class="math inline">\(\sigma^2\)</span> variance. Assume
<span class="math inline">\(\epsilon\)</span> is independent of <span
class="math inline">\({\bf x}\)</span> and of all other <span
class="math inline">\(\epsilon\)</span>'s. If linear regression is
carried out using a training data set <span
class="math inline">\(\mathcal{D} = \{(\mathbf{x}_1, y_1), \ldots, ({\bf
x}_N, y_N)\}\)</span>, and outputs the parameter vector <span
class="math inline">\(\mathbf{w}_{\rm lin}\)</span>, it can be shown
that the expected in-sample error <span class="math inline">\(E_{\rm
in}\)</span> with respect to <span
class="math inline">\(\mathcal{D}\)</span> is given by:</strong> <span
class="math display">\[
\mathbb{E}_{\mathcal{D} }[E_{\rm in}(\mathbf{w}_{\rm lin})] =
\sigma^2\left(1 - \frac{d + 1}{N}\right)
\]</span> For <span class="math inline">\(\sigma = 0.1\)</span> and
<span class="math inline">\(d = 8\)</span>, which among the following
choices is the smallest number of examples <span
class="math inline">\(N\)</span> that will result in an expected <span
class="math inline">\(E_{\rm in}\)</span> greater than 0.008?</p>
<ol type="a">
<li><p>10</p></li>
<li><p>25</p></li>
<li><p>100</p></li>
<li><p>500</p></li>
<li><p>1000</p></li>
</ol>
<p>这道题目中，已经给出了<span
class="math inline">\(E_{in}\)</span>的期望值如何计算，只需要将<span
class="math inline">\(\sigma = 0.1,d =
8\)</span>带入上式即可，算出来的是<span class="math inline">\(N =
45\)</span>的时候，<span
class="math inline">\(E_{in}\)</span>的期望值为0.008，而为了使得期望值变得更大，<span
class="math inline">\(N\)</span>的值也要变得更大，因此上面选项中最小的大于45的<span
class="math inline">\(N\)</span>是100，选c（Note:Greater意思是更大，而不是更好）.</p>
<p><strong>2. Recall that we have introduced the hat matrix <span
class="math inline">\(\mathrm{H} =
\mathrm{X}(\mathrm{X}^T\mathrm{X})^{-1}\mathrm{X}^T\)</span> in class,
where <span class="math inline">\(\mathrm{X} \in \mathbb{R}^{N\times
(d+1)}\)</span> containing <span class="math inline">\(N\)</span>
examples with <span class="math inline">\(d\)</span> features. Assume
<span class="math inline">\(\mathrm{X}^T\mathrm{X}\)</span> is
invertible and <span class="math inline">\(N &gt; d+1\)</span>, which
statement of <span class="math inline">\(\mathrm{H}\)</span> is
true?</strong></p>
<ol type="a">
<li><p>none of the other choices</p></li>
<li><p><span class="math inline">\(\mathrm{H}^{1126} =
\mathrm{H}\)</span></p></li>
<li><p><span class="math inline">\((d+1)\)</span> eigenvalues of <span
class="math inline">\(\mathrm{H}\)</span> are bigger than 1.</p></li>
<li><p><span class="math inline">\(N - (d+1)\)</span> eigenvalues of
<span class="math inline">\(\mathrm{H}\)</span> are 1</p></li>
<li><p><span class="math inline">\(\mathrm{H}\)</span> is always
invertible</p></li>
</ol>
<p>从<a
target="_blank" rel="noopener" href="https://wlsdzyzl.top/2018/08/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94linear-regression/#more">linear
regression</a>中，我们直到<span
class="math inline">\(H\)</span>矩阵的作用是在<span
class="math inline">\(X\)</span>空间做<span
class="math inline">\(Y\)</span>的投影，来得到<span
class="math inline">\(Y&#39;\)</span>，而投影一次与投影10次没什么太大区别，因此b是正确的。<span
class="math inline">\(H\)</span>的自由度是<span class="math inline">\(N
- (d+1)\)</span>，而它的特征值应该是有(N -
d+1)个不为0，而且特征值也不是一定的，只是与特征向量成比例，因此c，d是不对的。</p>
<p><strong>3. Which of the following is an upper bound of <span
class="math inline">\([[sign(w^Tx)≠y]]\)</span> for <span
class="math inline">\(y \in \{-1, +1\}\)</span>?</strong></p>
<ol type="a">
<li><p><span class="math inline">\(err(W) = \frac 1 2 e
^{(-yW^TX)}\)</span></p></li>
<li><p><span class="math inline">\(err(W) = [W^TX \geq
y]\)</span></p></li>
<li><p><span class="math inline">\(err(W) = max(0,1 -
yW^TX)\)</span></p></li>
<li><p><span class="math inline">\(err(W) = max(0,
-yW^TX)\)</span></p></li>
<li><p>none of the other choices</p></li>
</ol>
<p>这个题目最直观的看法依然是画图，a-red,b-blue,c-yellow,d-green,<span
class="math inline">\([sign(W^TX) \neq
y]\)</span>-black,按照上面的颜色画图如下： 当y = 1 时： <img
src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/8%5B%24IX%25Y9N86%5DB%2856AA%243TEE.png" />
当 y = -1时： <img
src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/%7BAK_J8U02WEX%247%5BMUS3O1%24B.png" />
从上面的图中我们可以很直观看到只有yellow线是符合的，因此这个题目答案为c.</p>
<p><strong>4. Which of the following is a differentiable function of
<span class="math inline">\(\mathbf{w}\)</span> everywhere?</strong></p>
<ol type="a">
<li><p><span class="math inline">\(err(W) = \frac 1 2 e
^{(-yW^TX)}\)</span></p></li>
<li><p><span class="math inline">\(err(W) = [W^TX \geq
y]\)</span></p></li>
<li><p><span class="math inline">\(err(W) = max(0,1 -
yW^TX)\)</span></p></li>
<li><p><span class="math inline">\(err(W) = max(0,
-yW^TX)\)</span></p></li>
<li><p>none of the other choices</p></li>
</ol>
<p>differentiable function 意思是可微函数。所以很明显答案是a.</p>
<p><strong>5. When using SGD on the following error functions and
`ignoring' some singular points that are not differentiable, which of
the following error function results in PLA?</strong></p>
<ol type="a">
<li><p><span class="math inline">\(err(W) = \frac 1 2 e
^{(-yW^TX)}\)</span></p></li>
<li><p><span class="math inline">\(err(W) = [W^TX \geq
y]\)</span></p></li>
<li><p><span class="math inline">\(err(W) = max(0,1 -
yW^TX)\)</span></p></li>
<li><p><span class="math inline">\(err(W) = max(0,
-yW^TX)\)</span></p></li>
<li><p>none of the other choices</p></li>
</ol>
<p>PLA更新策略： <span class="math display">\[
W_{n+1} = W_{n} + yX
\]</span> 使用SGD，也就是随机选择一个样本求<span
class="math inline">\(err(W)\)</span>的导数来更新。对于梯度下降中，对于下一步的做法是：
<span class="math display">\[
W_{n+1} = W_{n} - \alpha \frac {d_{err{W} } }{d_W}
\]</span> 其中<span
class="math inline">\(\alpha\)</span>是学习率，这道题目中应该为1.</p>
<p>因此主要是要求得导数。实际上，我们从上面的题目中可以轻易的看出，对于
a,b一定是不正确的，而c，d的导数是一样的：<span
class="math inline">\(\frac {d_{err{W} } }{d_W} =
-yX\)</span>，因此就更新的步骤来说，它们两个都是合适的。</p>
<p>但是我们不能都选上，要注意PLA算法做更新的时候是找到错误的点，如果点是正确的，我们则不应该更新。而c选项是PLA的Upper
bound，从上面题目的图中也可以看到，如果<span class="math inline">\(y =
-1，W^TX \in
[-1,1]\)</span>之间PLA是不会更新的，而c在这个时候依然会选择更新。所以这个题目的正确答案是d.</p>
<p>For Questions 6-10, consider a function <span
class="math inline">\(E(u,v) = e^u + e^{2v} + e^{uv} + u^2 - 2 u v + 2
v^2 - 3 u - 2 v\)</span>.</p>
<p><strong>6. What is the gradient <span class="math inline">\(\nabla
E(u, v)\)</span> around <span class="math inline">\((u, v)=(0,
0)\)</span>?</strong></p>
<ol type="a">
<li><p><span class="math inline">\((0,-2)\)</span></p></li>
<li><p>none of the other choices</p></li>
<li><p><span class="math inline">\((-3,1)\)</span></p></li>
<li><p><span class="math inline">\((3,-1)\)</span></p></li>
<li><p><span class="math inline">\((-2,0)\)</span></p></li>
</ol>
<p>这道题目不算难。 <span class="math inline">\(\nabla E(u,v) =
(e^u+ve^{uv}+2u - 2v - 3, 2e^{2v}+ue^{uv}-2u +4v -
2)\)</span>，代入<span class="math inline">\((0，0)\)</span>得到<span
class="math inline">\((-2,0)\)</span>，答案为e.</p>
<p><strong>7. In class, we have taught that the update rule of the
gradient descent algorithm is <span class="math inline">\((u_{t+1},
v_{t+1}) = (u_t, v_t) - \eta \nabla E(u_t, v_t)\)</span>. Please start
from <span class="math inline">\((u_0, v_0) = (0, 0)\)</span>, and fix
<span class="math inline">\(\eta=0.01\)</span>. What is <span
class="math inline">\(E(u_{5}, v_{5})\)</span> after five
updates?</strong></p>
<ol type="a">
<li><p>4.904</p></li>
<li><p>3.277</p></li>
<li><p>2.825</p></li>
<li><p>2.361</p></li>
<li><p>1.436</p></li>
</ol>
<p>这个题目虽然迭代次数不多，但是因为有<span
class="math inline">\(\exp\)</span>函数计算，还是比较复杂的。因此我编写梯度下降的程序来计算这道题目。</p>
<p>函数gradient_decent要传入4个参数：start开始值，get_gradicent
一个计算梯度的函数，eta学习率，i_times为迭代次数。</p>
<p>其他的函数包括计算梯度，更新W（update），以及计算<span
class="math inline">\(err\)</span>，程序代码如下： <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_gradient</span>(<span class="params">para</span>):</span><br><span class="line">    <span class="keyword">return</span> [math.exp(para[<span class="number">0</span>])+para[<span class="number">1</span>]*math.exp(para[<span class="number">0</span>]*para[<span class="number">1</span>])+<span class="number">2</span>*para[<span class="number">0</span>]-<span class="number">2</span>*para[<span class="number">1</span>]-<span class="number">3</span>,<span class="number">2</span>*math.exp(<span class="number">2</span>*para[<span class="number">1</span>])+para[<span class="number">0</span>]*math.exp(para[<span class="number">0</span>]*para[<span class="number">1</span>] )- <span class="number">2</span>*para[<span class="number">0</span>]+<span class="number">4</span>*para[<span class="number">1</span>]-<span class="number">2</span>]</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_decent</span>(<span class="params">start,get_gradient,eta,i_times</span>):</span><br><span class="line">    last = copy.deepcopy(start)</span><br><span class="line">    <span class="keyword">while</span>(i_times&gt;<span class="number">0</span>):</span><br><span class="line">        i_times-=<span class="number">1</span></span><br><span class="line">        update(last,get_gradient(last),eta)</span><br><span class="line">    <span class="keyword">return</span> last</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">last,gradient,eta</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span>  <span class="built_in">range</span>(<span class="built_in">len</span>(last)):</span><br><span class="line">        last[i] = last[i] - eta * gradient[i]</span><br><span class="line">    <span class="keyword">return</span> last</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Ein</span>(<span class="params">para</span>):</span><br><span class="line">    u = para[<span class="number">0</span>]</span><br><span class="line">    v = para[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> math.exp(u)+math.exp(<span class="number">2</span>*v)+ math.exp(u*v)+math.<span class="built_in">pow</span>(u,<span class="number">2</span>)-<span class="number">2</span>*u*v+<span class="number">2</span>*math.<span class="built_in">pow</span>(v,<span class="number">2</span>)-<span class="number">3</span>*u - <span class="number">2</span>*v</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="built_in">print</span>(gradient_decent([<span class="number">0</span>,<span class="number">0</span>],get_gradient,<span class="number">0.01</span>,<span class="number">5</span>))</span><br><span class="line">    <span class="built_in">print</span>(Ein(gradient_decent([<span class="number">0</span>,<span class="number">0</span>],get_gradient,<span class="number">0.01</span>,<span class="number">5</span>)))</span><br></pre></td></tr></table></figure></p>
<p>最后输出： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[0.09413996302028127, 0.0017891105951028273]</span><br><span class="line">2.8250003566832635</span><br></pre></td></tr></table></figure> 因此答案选c.</p>
<p><strong>8. Continuing from Question 7. If we approximate the <span
class="math inline">\(E(u + \Delta u, v + \Delta v)\)</span> by <span
class="math inline">\(\hat{E}_2(\Delta u, \Delta v)\)</span>, where
<span class="math inline">\(\hat{E}_2\)</span> is the second-order
Taylor's expansion of <span class="math inline">\(E\)</span> around
<span class="math inline">\((u,v)\)</span>. Suppose $<em>2(u, v) =
b</em>{uu} (u)^2 + b_{vv} (v)^2 + b_{uv} (u)(v) + b_u u + b_v v + b $,
What are the values of <span class="math inline">\((b_{uu}, b_{vv},
b_{uv}, b_u, b_v, b)\)</span> around <span class="math inline">\((u, v)
= (0, 0)\)</span>?</strong></p>
<ol type="a">
<li><p>none of the other choices</p></li>
<li><p><span class="math inline">\((3,8,-1,-2,0,3)\)</span></p></li>
<li><p><span class="math inline">\((3,8,-0.5,-1,-2,0)\)</span></p></li>
<li><p><span
class="math inline">\((1.5,4,-0.5,-1,-2,0)\)</span></p></li>
<li><p><span class="math inline">\((1.5,4,-1,-2,0,3)\)</span></p></li>
</ol>
<p>这个题目不算难，实际上是一个多维函数的二阶泰勒展开，而<span
class="math inline">\(b{_uu}\)</span>则是对<span
class="math inline">\(u\)</span>求二阶偏导再除以<span
class="math inline">\(2!\)</span>。只要一项项计算上面的值就可以了。
<span class="math inline">\(b_{uu} = 3,b_{vv} = 8,b_{uv} = -1,b_{u} =
-2,b_{v} = 0,b = 3\)</span>，二元泰勒展开因此答案是e.</p>
<p><strong>9. Continue from Question 8 and denote the Hessian matrix to
be <span class="math inline">\(\nabla^2 E(u, v)\)</span>, and assume
that the Hessian matrix is positive definite. What is the optimal <span
class="math inline">\((\Delta u, \Delta v)\)</span> to minimize <span
class="math inline">\(\hat{E}_2(\Delta u, \Delta v)\)</span>? (The
direction is called the Newton Direction.)</strong></p>
<ol type="a">
<li><p><span class="math inline">\(+(\nabla^2E(u,v))^{-1}\nabla
E(u,v)\)</span></p></li>
<li><p><span class="math inline">\(-(\nabla^2E(u,v))^{-1}\nabla
E(u,v)\)</span></p></li>
<li><p>none of the other choices</p></li>
<li><p><span class="math inline">\(+\nabla ^2 E(u,v) \nabla
E(u,v)\)</span></p></li>
<li><p><span class="math inline">\(-\nabla ^2 E(u,v) \nabla
E(u,v)\)</span></p></li>
</ol>
<p>其实这个题目我不是很清楚。这其中涉及到了海森矩阵以及牛顿方向。通过简单的了解，如果一个点的梯度为0向量，而且在该点的海森矩阵为正定矩阵，那么该点为极小值点。题目中说明了海森矩阵为正定矩阵，因此找这个极值点，如果不知道牛顿方向，我就会按照梯度下降的来。而查阅了牛顿方向之后，牛顿方向就是e选项。所以选择b.</p>
<p>当然，这个题目实际上就是搜出来的答案，真正想要理解需要更加深入地去看线性代数。</p>
<p><strong>10. Use the Newton direction (without ) for updating and
start from <span class="math inline">\((u_0, v_0) = (0, 0)\)</span>.
What is <span class="math inline">\(E(u_{5}, v_{5})\)</span> after five
updates?</strong></p>
<ol type="a">
<li><p>4.904</p></li>
<li><p>3.277</p></li>
<li><p>2.825</p></li>
<li><p>2.361</p></li>
<li><p>1.436</p></li>
</ol>
<p>这道题目，当然我还是想借助程序来完成，不过这次的程序相比之前会更麻烦一点。因为之前设置好了接口，也就是在梯度下降的时候传入求得梯度的函数，因此这里我们只需要写出计算
<span class="math inline">\(\nabla ^2 E(u,v) \nabla
E(u,v)\)</span>的函数： <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Newton_direction</span>(<span class="params">para</span>):</span><br><span class="line">    u = para[<span class="number">0</span>]</span><br><span class="line">    v = para[<span class="number">1</span>]</span><br><span class="line">    hs = [[math.exp(u)+math.<span class="built_in">pow</span>(v,<span class="number">2</span>)*math.exp(u*v)+<span class="number">2</span>,math.exp(u*v)+v*u*math.exp(u*v)-<span class="number">2</span>],</span><br><span class="line">          [math.exp(u*v)+v*u*math.exp(u*v)-<span class="number">2</span>,<span class="number">4</span>*math.exp(<span class="number">2</span>*v)+math.<span class="built_in">pow</span>(u,<span class="number">2</span>)*math.exp(u*v)+<span class="number">4</span>]]</span><br><span class="line">    m = mat(hs)</span><br><span class="line">    <span class="keyword">return</span> (m.I*mat([get_gradient([u,v])]).T).T.tolist()[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
最后在主函数中修改传入的学习率与求梯度的函数： <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(Ein(gradient_decent([<span class="number">0</span>,<span class="number">0</span>],Newton_direction,<span class="number">1</span>,<span class="number">5</span>)))</span><br></pre></td></tr></table></figure>
得到结果：<code>2.360823345643139</code> 因此答案选d.</p>
<p><strong>11. Consider six inputs <span
class="math inline">\(\mathbf{x}_1 = (1, 1)\)</span>, <span
class="math inline">\(\mathbf{x}_2 = (1, -1)\)</span>, <span
class="math inline">\(\mathbf{x}_3 = (-1, -1)\)</span>, <span
class="math inline">\(\mathbf{x}_4 = (-1, 1)\)</span>, <span
class="math inline">\(\mathbf{x}_5 = (0, 0)\)</span>, <span
class="math inline">\(\mathbf{x}_6 = (1, 0)\)</span>. What is the
biggest subset of those input vectors that can be shattered by the union
of quadratic, linear, or constant hypotheses of <span
class="math inline">\(\mathbf{x}\)</span>?</strong></p>
<ol type="a">
<li><p><span
class="math inline">\(\mathbf{x}_1,\mathbf{x}_2,\mathbf{x}_3\)</span></p></li>
<li><p><span
class="math inline">\(\mathbf{x}_1,\mathbf{x}_2,\mathbf{x}_3,\mathbf{x}_4,\mathbf{x}_5,\mathbf{x}_6\)</span></p></li>
<li><p><span
class="math inline">\(\mathbf{x}_1,\mathbf{x}_2,\mathbf{x}_3,\mathbf{x}_4\)</span></p></li>
<li><p><span
class="math inline">\(\mathbf{x}_1,\mathbf{x}_3\)</span></p></li>
<li><p><span
class="math inline">\(\mathbf{x}_1,\mathbf{x}_2,\mathbf{x}_3,\mathbf{x}_4,\mathbf{x}_5\)</span></p></li>
</ol>
<p>首先，我们可以看到特征量的个数是2（<span
class="math inline">\(d=2\)</span>），而由前面的推导可以知道，最高次为2次时候，它可以组成的新的特征值的个数为5，而它的vc
dimension是小于等于6的.一种可行的办法是将x空间投影到z空间以后，用线性的办法来shatter这6个点，再返回看原来的x组合而成的特征是否能够满足这6个特征量。但是实际上这还是复杂的。也可以利用梯度下降，跑程序来对上述选项来进行排除。具体答案是多少还需要进一步进行排除。这个答案是b.可以shatter6个点.</p>
<p><strong>12. Assume that a transformer peeks the data and decides the
following transform <span
class="math inline">\(\boldsymbol{\Phi}\)</span> "intelligently" from
the data of size <span class="math inline">\(N\)</span>. The transform
maps <span class="math inline">\(\mathbf{x} \in \mathbb{R}^d\)</span> to
<span class="math inline">\(\mathbf{z} \in \mathbb{R}^N\)</span>,
where</strong> <span class="math display">\[
(\boldsymbol{\Phi}(\mathbf{x}))_n = z_n = \left[\left[ \mathbf{x} =
\mathbf{x}_n \right]\right]
\]</span> Consider a learning algorithm that performs PLA after the
feature transform.Assume that all <span
class="math inline">\(\mathbf{x}_n\)</span> are different, <span
class="math inline">\(30%\)</span> of the <span
class="math inline">\(y_n\)</span>'s are positive, and <span
class="math inline">\(sign(0)=+1\)</span>. Then, estimate the <span
class="math inline">\(E_{out}\)</span> of the algorithm with a test set
with all its input vectors <span
class="math inline">\(\mathbf{x}\)</span> different from those training
<span class="math inline">\(\mathbf{x}_n\)</span>'s and <span
class="math inline">\(30%\)</span> of its output labels <span
class="math inline">\(y\)</span> to be positive. Which of the following
is not true?</p>
<ol type="a">
<li><p>PLA will halt after enough iterations.</p></li>
<li><p><span class="math inline">\(E_{out} = 0.7\)</span></p></li>
<li><p><span class="math inline">\(E_{in} = 0.7\)</span></p></li>
<li><p>All <span class="math inline">\(\mathbf{Z}_n\)</span>'s are
orthogonal to each other.</p></li>
<li><p>The transformed data set is always linearly separable in the
<span class="math inline">\(\mathcal{Z}\)</span> space.</p></li>
</ol>
<p>要明白这个题目首先要读懂题意。
题目中对原来的向量进行了特征转换，原来向量为d维，新的向量为N维，每一个样本对应的新的Z向量是
<span class="math inline">\([ [[x_i = x_1]], [[x_i = x_2]],...,[[x_i =
x_n]]]^T\)</span>,题目中已经告知，<span
class="math inline">\(X_n\)</span>是各不相同的，因此我们可以直到<span
class="math inline">\(Z\)</span>向量会类似于：<span
class="math inline">\([1,0,...,0]^T,[0,1,...,0]^T,...,[0,0,...,1]\)</span>.在这样的样本上进行PLA算法。我们可以知道这种情况是最好的情况，是可以被shatter的，PLA一定会停止，而且他们是互相正交的。因此答案就在b，c中选择。而既然PLA会停止说明找到了最小的<span
class="math inline">\(E_{in}\)</span>，此时<span
class="math inline">\(E_{in}\)</span>为0.因此答案选c.</p>
<p>至于c为何是正确的，首先，因为测试集的样本与原来的训练集样本也没有一致的，因此他们得到的转换后的z向量为n维0向量.而题目中说明了<span
class="math inline">\(sign(0)=+1\)</span>，因此它会将所有的测试样本都判别为positive，而实际上只有30%为positive，因此测试得到的<span
class="math inline">\(E_{out}\)</span>为70%.</p>
<p>For Questions 13-15, consider the target function: <span
class="math display">\[
f(x_1, x_2) = \mbox{sign}(x_1^2 + x_2^2 - 0.6)
\]</span></p>
<p><strong>13. Generate a training set of N = 1000N=1000 points on <span
class="math inline">\(X=[−1,1]\)</span> with uniform probability of
picking each <span class="math inline">\(\mathbf{x} \in
\mathcal{X}\)</span>. Generate simulated noise by flipping the sign of
the output in a random <span class="math inline">\(10\%\)</span> subset
of the generated training set.</strong></p>
<p>Carry out Linear Regression without transformation, i.e., with
feature vector: <span class="math display">\[
(1,x_1,x_2)
\]</span></p>
<p>to find the weight <span class="math inline">\(\mathbf{w}_{\rm
lin}\)</span>, and use <span class="math inline">\(\mathbf{w}_{\rm
lin}\)</span> directly for classification. What is the closest value to
the classification <span class="math inline">\((0/1)\)</span> in-sample
error (<span class="math inline">\(E_{\rm in}\)</span>)? Run the
experiment <span class="math inline">\(1000\)</span> times and take the
average <span class="math inline">\(E_{\rm in}\)</span> in order to
reduce variation in your results.</p>
<ol type="a">
<li><p>0.1</p></li>
<li><p>0.3</p></li>
<li><p>0.5</p></li>
<li><p>0.7</p></li>
<li><p>0.9</p></li>
</ol>
<p>这次作业与之前不一样了，很早就遇到编程题目了，而且前面有几道题目虽然不是编程题目，我也依然是用程序计算出来的。</p>
<p>这个题目想要用程序解答其实也不是非常难。在这里我使用的是一步求法，而不是梯度下降那样迭代。首先是要生成数据：
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_data</span>():</span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">        x1 = random.random()*<span class="number">2</span>-<span class="number">1</span></span><br><span class="line">        x2 = random.random()*<span class="number">2</span>-<span class="number">1</span></span><br><span class="line">        result.append([x1,x2,sign(<span class="built_in">pow</span>(x1,<span class="number">2</span>)+<span class="built_in">pow</span>(x2,<span class="number">2</span>)-<span class="number">0.6</span>)])</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure></p>
<p>为了检查做的数据是否正确，对生成的数据进行了可视化（因为输出长宽比例不同，所以这个圆看上去不是正圆）：
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">visualize</span>(<span class="params">data,W=[]</span>):</span><br><span class="line">    nx = []</span><br><span class="line">    ny = []</span><br><span class="line">    ox = []</span><br><span class="line">    oy = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(data)):</span><br><span class="line">        <span class="keyword">if</span> data[i][-<span class="number">1</span>] == -<span class="number">1</span>:</span><br><span class="line">            nx.append(data[i][<span class="number">1</span>])</span><br><span class="line">            ny.append(data[i][<span class="number">2</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            ox.append(data[i][<span class="number">1</span>])</span><br><span class="line">            oy.append(data[i][<span class="number">2</span>])</span><br><span class="line">    plt.scatter(nx,ny,marker=<span class="string">&quot;x&quot;</span>,c=<span class="string">&quot;r&quot;</span>)</span><br><span class="line">    plt.scatter(ox,oy,marker=<span class="string">&quot;o&quot;</span>,c=<span class="string">&quot;g&quot;</span>)</span><br><span class="line">    theta = np.linspace(<span class="number">0</span>, <span class="number">2</span> * np.pi, <span class="number">800</span>)</span><br><span class="line">    x, y = np.cos(theta) * <span class="number">0.77459666924148</span>, np.sin(theta) * <span class="number">0.77459666924148</span><span class="comment"># sqrt 0.6</span></span><br><span class="line">    plt.plot(x, y, color=<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(W)!=<span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(W)</span><br><span class="line">        x = np.linspace(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">50</span>)</span><br><span class="line">        y = -W[<span class="number">1</span>] / W[<span class="number">2</span>] * x - W[<span class="number">0</span>] / W[<span class="number">2</span>]</span><br><span class="line">        plt.plot(x, y, color=<span class="string">&quot;black&quot;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure> <img
src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/RE%40%60F%29JRPQT_KE%7EFIVEC17K.png"
alt="生成数据" /></p>
<p>然后通过线性回归得到<span class="math inline">\(W =
(X^TX)^{-1}X^TY\)</span>: <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">linear_regression_one_step</span>(<span class="params">data</span>):</span><br><span class="line">    X_matrix = []</span><br><span class="line">    Y_matrix = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(data)):</span><br><span class="line">        temp = []</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(data[i])-<span class="number">1</span>):</span><br><span class="line">            temp.append(data[i][j])</span><br><span class="line">        X_matrix.append(temp)</span><br><span class="line">        Y_matrix.append([data[i][-<span class="number">1</span>]])</span><br><span class="line">    X = np.mat(X_matrix)</span><br><span class="line">    Y = np.mat(Y_matrix)</span><br><span class="line"></span><br><span class="line">    W = (X.T*X).I*X.T*Y</span><br><span class="line">    <span class="keyword">return</span> W.T.tolist()[<span class="number">0</span>]</span><br></pre></td></tr></table></figure></p>
<p>最后通过分类计算<span class="math inline">\(E_{in}\)</span>：
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Ein</span>(<span class="params">data,W</span>):</span><br><span class="line">    err_num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(data)):</span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(W)):</span><br><span class="line">            res += W[j]*data[i][j]</span><br><span class="line">        <span class="keyword">if</span> sign(res) != data[i][-<span class="number">1</span>]:</span><br><span class="line">            err_num+=<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> err_num</span><br></pre></td></tr></table></figure></p>
<p>运行1000次取输出为：<code>0.50614</code></p>
<p>因此答案选c.其实我们也可以想象得到这个错误率是接近0.5的，可视化的结果如下：
<img
src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/_%7BA%60P%28N%255%7B4U8%25%25NQ%24H%408O9.png" /></p>
<p>不管怎么画这条线，总会有接近一半的区域被分错.</p>
<p><strong>14. Now, transform the training data into the following
nonlinear feature vector:<span
class="math inline">\((1,x_1,x_2,x_1x_2,x_1^2,x_2^2)\)</span>.Find the
vector <span class="math inline">\(\tilde{\mathbf{w} }\)</span> that
corresponds to the solution of Linear Regression, and take it for
classification. Which of the following hypotheses is closest to the one
you find using Linear Regression on the transformed input? Closest here
means agrees the most with your hypothesis (has the most probability of
agreeing on a randomly selected point).</strong></p>
<ol type="a">
<li><p><span class="math inline">\(g(x_1,x_2) =
sign(-1-1.5x_1+0.08x_2+0.13x_1x_2+0.05x_1^2+1.5x_2^2)\)</span></p></li>
<li><p><span class="math inline">\(g(x_1,x_2) =
sign(-1-1.5x_1+0.08x_2+0.13x_1x_2+0.05x_1^2+0.05x_2^2)\)</span></p></li>
<li><p><span class="math inline">\(g(x_1,x_2) =
sign(-1-0.05x_1+0.08x_2+0.13x_1x_2+1.5x_1^2+15x_2^2)\)</span></p></li>
<li><p><span class="math inline">\(g(x_1,x_2) =
sign(-1-0.05x_1+0.08x_2+0.13x_1x_2+1.5x_1^2+1.5x_2^2)\)</span></p></li>
<li><p><span class="math inline">\(g(x_1,x_2) =
sign(-1-0.05x_1+0.08x_2+0.13x_1x_2+15x_1^2+1.5x_2^2)\)</span></p></li>
</ol>
<p>经过特征转换后进行线性回归，找到最相似的W向量。实际上，每次输出的向量不容易看出来他们的相似性，最好的办法就是从z空间转回去看实际的分类效果。
为了完成这道题目，需要添加一些新的函数。首先是一个特征转换的函数：
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">transform_data</span>(<span class="params">data</span>):</span><br><span class="line">    t_data = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(data)):</span><br><span class="line">        temp = [<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        temp.append(data[i][<span class="number">1</span>])</span><br><span class="line">        temp.append(data[i][<span class="number">2</span>])</span><br><span class="line">        temp.append(data[i][<span class="number">1</span>]*data[i][<span class="number">2</span>])</span><br><span class="line">        temp.append(data[i][<span class="number">1</span>]*data[i][<span class="number">1</span>])</span><br><span class="line">        temp.append(data[i][<span class="number">2</span>]*data[i][<span class="number">2</span>])</span><br><span class="line">        temp.append(data[i][-<span class="number">1</span>])</span><br><span class="line">        t_data.append(temp)</span><br><span class="line">    <span class="keyword">return</span> t_data</span><br></pre></td></tr></table></figure>
另外，我重新写了一个可视化的函数，来画出学习后分类的结果，并且与原始的界限做对比。实际上，如果不通过可视化，单单从W向量我们不是很容易能看出来他们之间的相似度，如：
<span class="math display">\[
W = [-1.233493970901365, 0.0289716107001069, -0.04249028735529122,
-0.06563216707578017, 1.8906604891218894, 2.014485177474217]
\]</span> 得到的分类效果如下： <img
src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/%24NF51V%5BFC0ELUWH8QXJ%7D%5BDE.png" /></p>
<p>而其他几个选项按照a,b,c,d,e的顺序分别得到下面的分类效果： <img
src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/XZSCVFU97%28EH4V5%29X%5DDT%7E70.png" />
<img
src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/V%28%7D%7BZ%5BQ2L%402145%5D%7D%5D9FW%7E_B.png" />
<img
src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/R67TAG1%7BEXKD2%406Y_WS%28SZA.png" />
<img
src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/%5BI13AGR586%24DR9%5BG3%5D038%40F.png" />
<img
src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/2HM1%5D84NO9J%29%7BUA%7DNI%7E2%5D90.png" /></p>
<p>可以很明显的看出来，答案选择d.</p>
<p><strong>15. Following Question 14, what is the closest value to the
classification out-of-sample error <span class="math inline">\(E_{\rm
out}\)</span> of your hypothesis? Estimate it by generating a new set of
1000 points and adding noise as before. Average over 1000 runs to reduce
the variation in your results.</strong></p>
<ol type="a">
<li><p>0.1</p></li>
<li><p>0.3</p></li>
<li><p>0.5</p></li>
<li><p>0.7</p></li>
<li><p>0.9</p></li>
</ol>
<p>这道题目需要来测量<span
class="math inline">\(E_{out}\)</span>，并且要加上噪声。其实还是很简单的，只需要做简单的改动，来计算<span
class="math inline">\(E_{out}\)</span>.</p>
<p>既然是<span
class="math inline">\(E_{out}\)</span>，那就需要每次都重新生成数据，这就是和<span
class="math inline">\(E_{in}\)</span>的区别，再加个转换，除此之外是一样的，因此可以复用<span
class="math inline">\(E_{in}\)</span>函数： <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Eout</span>(<span class="params">W</span>):</span><br><span class="line">    data = make_data()</span><br><span class="line">    t_data = transform_data(data)</span><br><span class="line">    <span class="keyword">return</span> Ein(t_data,W)</span><br></pre></td></tr></table></figure></p>
<p>然后运行1000次就可以了，最后得到的错误率：<code>0.126856</code>,答案选a.</p>
<p>For Questions 16-17, you will derive an algorithm for the multinomial
(multiclass) logistic regression model. <strong>16. For a <span
class="math inline">\(K\)</span>-class classification problem, we will
denote the output space <span class="math inline">\(\mathcal{Y} = \{1,
2, \ldots, K\}\)</span>. The hypotheses considered by the model are
indexed by a list of weight vectors <span
class="math inline">\((\mathbf{w}_1, \mathbf{w}_2, \ldots,
\mathbf{w}_K)\)</span>, each weight vector of length <span
class="math inline">\(d+1\)</span>. Each list represents a
hypothesis</strong> <span class="math display">\[
h_y( X) = \frac {e^{W_y^TX} } {\sum _{k=1} ^{K} e^{W_k^TX} }
\]</span> that can be used to approximate the target distribution <span
class="math inline">\(P(y|\mathbf{x})\)</span>P. The model then seeks
for the maximum likelihood solution over all such hypotheses.(Note:<span
class="math inline">\(X\)</span> = <span
class="math inline">\(\mathbf{x}\)</span>)</p>
<p>For general <span class="math inline">\(K\)</span>, derive an <span
class="math inline">\(E_{\text{in} }(\mathbf{w}_1, \cdots,
\mathbf{w}_K)\)</span> like page 11 of Lecture 10 slides by minimizing
the negative log likelihood. What is the resulting <span
class="math inline">\(E_{\text{in} }\)</span>?</p>
<ol type="a">
<li><p> N <em>{n = 1}^N ( </em>{k=1} ^K (W_k^TX_n -
W_{y_n}^TX_n))</p></li>
<li><p> N <em>{n = 1}^N (</em>{k=1} ^K W_k^TX_n - W_{y_n}^TX_n)</p></li>
<li><p> N <em>{n = 1}^N ( (</em>{k=1} ^K e<sup>{W_k</sup>TX_n}) -
W_{y_n}^TX_n)</p></li>
<li><p>none of the other choices</p></li>
<li><p> N <em>{n = 1}^N ( (</em>{k=1} ^K e<sup>{W_k</sup>TX_n} -
e<sup>{W_{y_n}</sup>TX_n}))</p></li>
</ol>
<p>这是一个很有意思的概率的定义，它保证了分到各个分类的概率和加起来为1，而之前的假设是不能保证这个问题的。</p>
<p>对于<span
class="math inline">\(E_{in}\)</span>的处理，也是一致的。按照之前的想法来一步一步做。首先，计算出在该<span
class="math inline">\(W\)</span>的情况下，出现当前分类的概率，可以知道的是互相连乘，最后为了让他变成加法，以及最小化（而非最大化），需要加一个负号，忽略P(X=X_i)。</p>
<p>可以得到下面的过程：</p>
<p><span class="math display">\[
\prod_{i=n}^{N} \frac {e^{W_{y_n}^TX_{n} } } {\sum _{k=1} ^{K}
e^{W_k^TX_n} }
\]</span></p>
<p><span class="math display">\[
=&gt; -\sum_{n = 1}^N( \ln (e^{W_{y_n}^TX_n}) - \ln (\sum _{k = 1}^K
e^{W_{k}^TX_n}))
\]</span></p>
<p><span class="math display">\[
  =&gt;\sum _{n = 1}^N \left ( \ln (\sum _{k=1} ^K e^{W_k^TX_n} -
W_{y_n}^TX_n)\right)
\]</span></p>
<p>最后除以N，得到选项c.</p>
<p><strong>17. For the <span class="math inline">\(E_{\text{in}
}\)</span> derived above, its gradient <span
class="math inline">\(\nabla E_{\text{in} }\)</span> can be represented
by <span class="math inline">\(\left(\frac{\partial E_{\text{in} }
}{\partial\mathbf{w}_1}, \frac{\partial E_{\text{in} }
}{\partial\mathbf{w}_2}, \cdots, \frac{\partial E_{\text{in} }
}{\partial\mathbf{w}_K}\right)\)</span>, write down <span
class="math inline">\(\frac{\partial E_{\text{in} }
}{\partial\mathbf{w}_i}\)</span> .</strong></p>
<ol type="a">
<li><p> N <em>{n=1}^N (</em>{i = 1} <sup>K(e</sup>{W_i^TX_n} -
[[y_n=i]])X_n)</p></li>
<li><p> N _{n=1}^N ((h_i(x_n)-1)x_n)</p></li>
<li><p>none of the other choices</p></li>
<li><p> N _{n=1}^N ((h_i(x_n) - [[y_n=i]])X_n)</p></li>
<li><p> N <em>{n=1}^N ( </em>(i =
1)<sup>K(e</sup>{W_i^TX_n}-1)X_n)</p></li>
</ol>
<p>这个题目就是对上面的题目的选择结果进行求导即可。虽然比较复杂，但是不算难。一步步算下来就成，正确答案是d.</p>
<p>For Questions 18-20, you will play with logistic regression. Please
use the following set for training:</p>
<p>https://www.csie.ntu.edu.tw/~htlin/mooc/datasets/mlfound_algo/hw3_train.dat</p>
<p>and the following set for testing:</p>
<p>https://www.csie.ntu.edu.tw/~htlin/mooc/datasets/mlfound_algo/hw3_test.dat</p>
<p><strong>18. Implement the fixed learning rate gradient descent
algorithm for logistic regression. Run the algorithm with <span
class="math inline">\(\eta = 0.001\)</span> and <span
class="math inline">\(T = 2000\)</span>. What is <span
class="math inline">\(E_{out}(g)\)</span> from your algorithm, evaluated
using the <span class="math inline">\(0/1\)</span> error on the test
set?</strong></p>
<ol type="a">
<li><p>0.475</p></li>
<li><p>0.412</p></li>
<li><p>0.322</p></li>
<li><p>0.220</p></li>
<li><p>0.103</p></li>
</ol>
<p><strong>19. Implement the fixed learning rate gradient descent
algorithm for logistic regression. Run the algorithm with <span
class="math inline">\(\eta = 0.01\)</span> and <span
class="math inline">\(T = 2000\)</span>. What is <span
class="math inline">\(E_{out}(g)\)</span> from your algorithm, evaluated
using the <span class="math inline">\(0/1\)</span> error on the test
set?</strong></p>
<ol type="a">
<li><p>0.475</p></li>
<li><p>0.412</p></li>
<li><p>0.322</p></li>
<li><p>0.220</p></li>
<li><p>0.103</p></li>
</ol>
<p><strong>20. Implement the fixed learning rate stochastic gradient
descent algorithm for logistic regression. Instead of randomly choosing
nn in each iteration, please simply pick the example with the cyclic
order <span class="math inline">\(n = 1, 2, \ldots, N, 1, 2,
\ldots\)</span>,Run the algorithm with <span class="math inline">\(\eta
= 0.001\)</span> and <span class="math inline">\(T = 2000\)</span>. What
is <span class="math inline">\(E_{out}(g)\)</span> from your algorithm,
evaluated using the <span class="math inline">\(0/1\)</span> error on
the test set?</strong></p>
<ol type="a">
<li><p>0.475</p></li>
<li><p>0.412</p></li>
<li><p>0.322</p></li>
<li><p>0.220</p></li>
<li><p>0.103</p></li>
</ol>
<p>这3道题目，可以用一套方法做出来，可以引用第7题的程序中的梯度下降算法与13题程序中的对于<span
class="math inline">\(E_{in}\)</span>的估计。当然，对于之前代码的还需要进行一丝修改。
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gradient_decent_7 <span class="keyword">as</span> gd</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span>  numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> _linear_regression_13 <span class="keyword">as</span> lr</span><br><span class="line">now = <span class="number">0</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">logistic</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+math.exp(-x))</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_logistic_gradient</span>(<span class="params">lastW,data</span>):</span><br><span class="line">    W = np.mat([lastW]).T</span><br><span class="line">    X = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(data)):</span><br><span class="line">        temp = data[i][<span class="number">0</span>:-<span class="number">1</span>]</span><br><span class="line">        temp.append(<span class="number">1</span>)</span><br><span class="line">        X.append(np.mat([temp]).T)</span><br><span class="line">    gra = logistic(-data[<span class="number">0</span>][-<span class="number">1</span>]*(W.T*X[<span class="number">0</span>])[<span class="number">0</span>][<span class="number">0</span>])*(-data[<span class="number">0</span>][-<span class="number">1</span>]*X[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="built_in">len</span>(data)):</span><br><span class="line">        gra = gra + logistic(-data[i][-<span class="number">1</span>]*(W.T*X[i])[<span class="number">0</span>][<span class="number">0</span>])*(-data[i][-<span class="number">1</span>]*X[i])</span><br><span class="line">    <span class="comment">#print(gra)</span></span><br><span class="line">    <span class="keyword">return</span> ((<span class="number">1</span>/<span class="built_in">len</span>(data))*gra).T.tolist()[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">stochastic_gradient_init</span>():</span><br><span class="line">    <span class="keyword">global</span>  now</span><br><span class="line">    now = <span class="number">0</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">stochastic_gradient</span>(<span class="params">lastW,data</span>):</span><br><span class="line">    <span class="keyword">global</span> now</span><br><span class="line">    <span class="keyword">if</span> now == <span class="built_in">len</span>(data):</span><br><span class="line">        now = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    W = np.mat([lastW]).T</span><br><span class="line">    temp = data[now][<span class="number">0</span>:-<span class="number">1</span>]</span><br><span class="line">    temp.append(<span class="number">1</span>)</span><br><span class="line">    X = np.mat([temp]).T</span><br><span class="line">    gra = logistic(-data[now][-<span class="number">1</span>]*(W.T*X)[<span class="number">0</span>][<span class="number">0</span>])*(-data[now][-<span class="number">1</span>]*X)</span><br><span class="line">    now += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> gra.T.tolist()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">readDataFrom</span>(<span class="params">filename</span>):</span><br><span class="line">    result = []</span><br><span class="line">    separator = re.<span class="built_in">compile</span>(<span class="string">&#x27;\t|\b| |\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(filename,<span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        line = f.readline()[<span class="number">1</span>:-<span class="number">1</span>]</span><br><span class="line">        <span class="comment">#print(line)</span></span><br><span class="line">        <span class="keyword">while</span> line:</span><br><span class="line">            temp = separator.split(line)</span><br><span class="line">            <span class="comment">#print(temp)</span></span><br><span class="line"></span><br><span class="line">            abc = [<span class="built_in">float</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> temp]</span><br><span class="line">            <span class="comment">#print(abc)</span></span><br><span class="line">            result.append(abc)</span><br><span class="line">            <span class="comment">#print(result)</span></span><br><span class="line">            line = f.readline()[<span class="number">1</span>:-<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
上面展示了5个函数，第一个logistic函数就是为了计算logistic函数的值，然后分别有两个计算梯度的函数，为了可以复用之前的代码，采用了一样的形式，不过值得注意的是多增加了一个data参数，这是之前没有的。为了实现题目中要求的顺序采取一个样本来做修正，使用了一个全局变量来记录当前的样本索引，同时有一个初始化函数，可以让样本索引归零。</p>
<p>还有一个readDataFrom不用多说，根据资料的输入形式进行适当调整。</p>
<p>最后在主函数中整合输出结果： <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    data = readDataFrom(<span class="string">&quot;./hw3_train.dat&quot;</span>)</span><br><span class="line">    start = [<span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="built_in">len</span>(data[<span class="number">0</span>]))]</span><br><span class="line">    i_times = <span class="number">2000</span></span><br><span class="line">    stochastic_gradient_init()</span><br><span class="line">    W1 = gd.gradient_decent(start,get_logistic_gradient,<span class="number">0.001</span>,i_times,data)</span><br><span class="line">    W2 = gd.gradient_decent(start,get_logistic_gradient,<span class="number">0.01</span>,i_times,data)</span><br><span class="line">    W3 = gd.gradient_decent(start,stochastic_gradient,<span class="number">0.001</span>,i_times,data)</span><br><span class="line">    err1 = lr.Ein(data,W1)</span><br><span class="line">    err2 = lr.Ein(data,W2)</span><br><span class="line">    err3 = lr.Ein(data,W3)</span><br><span class="line">    out_data = readDataFrom(<span class="string">&quot;./hw3_test.dat&quot;</span>)</span><br><span class="line">    err_out1 = lr.Ein(out_data,W1)</span><br><span class="line">    err_out2 = lr.Ein(out_data,W2)</span><br><span class="line">    err_out3 = lr.Ein(out_data,W3)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Ein:&quot;</span>,err1,err1/<span class="built_in">len</span>(data),err2,err2/<span class="built_in">len</span>(data),err3,err3/<span class="built_in">len</span>(data))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Eout:&quot;</span>,err_out1,err_out1/<span class="built_in">len</span>(out_data),err_out2,err_out2/<span class="built_in">len</span>(out_data),err_out3,err_out3/<span class="built_in">len</span>(out_data))</span><br></pre></td></tr></table></figure>
我将起始的W设定为0向量，不同的起始向量可能得到不同的性能。</p>
<p>最后的结果，分别是3道题目的<span
class="math inline">\({E_in}\)</span>与<span
class="math inline">\(E_{out}\)</span>.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Ein: 430 0.43 204 0.204 427 0.427</span><br><span class="line">Eout: 1354 0.4513333333333333 678 0.226 1352 0.45066666666666666</span><br></pre></td></tr></table></figure>
<p>因此我认为答案是a,d,a.
可以看到学习率过低的话可能会使得下降速度过慢，而且随机梯度下降的性能有时候不见得比普通的梯度下降差，但是速度却大大提高。</p>
<p>将迭代次数更新为10000，3个学习方法都取得了不错的效果，而步长大的性能反而不如之前，说明因为步长过大，它只能再最低点徘徊很难取得更低的位置，而步长小的依然没有达到极限：
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Ein: 236 0.236 206 0.206 235 0.235</span><br><span class="line">Eout: 779 0.25966666666666666 695 0.23166666666666666 776 0.25866666666666666</span><br></pre></td></tr></table></figure></p>
<h2 id="p.s.">p.s.</h2>
<p>1.python中的全局变量，list为引用，而赋值一个常数则会重新声明，为了避免歧义。因此想要改变全局变量的常数，需要添加关键词global。</p>
<p>2.牛顿方向与海森矩阵。</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/machine-learning/" rel="tag"># machine learning</a>
              <a href="/tags/homework/" rel="tag"># homework</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2018/09/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Nonlinear-Transformation/" rel="prev" title="机器学习——Nonlinear Transformation">
                  <i class="fa fa-angle-left"></i> 机器学习——Nonlinear Transformation
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2018/09/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Overfitting/" rel="next" title="机器学习——Overfitting">
                  机器学习——Overfitting <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2023</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">wlsdzyzl</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>




</body>
</html>
